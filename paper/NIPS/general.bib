Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Bergstra2011a,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap-proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos-sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu-ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex-pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli-able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Bergstra, James and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {2012arXiv1206.2944S},
eprint = {1206.2944},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra et al. - Unknown - Algorithms for Hyper-Parameter Optimization.pdf:pdf},
isbn = {9781618395993},
issn = {10495258},
pages = {2546--2554},
pmid = {9377276},
title = {{Algorithms for Hyper-Parameter Optimization}},
OPTurl = {https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
year = {2011}
}
@inproceedings{Smithson2016,
abstract = {Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 1020 solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.},
archivePrefix = {arXiv},
arxivId = {1611.02120},
author = {Smithson, Sean C. and Yang, Guang and Gross, Warren J. and Meyer, Brett H.},
booktitle = {Proceedings of the 35th International Conference on Computer-Aided Design - ICCAD '16},
doi = {10.1145/2966986.2967058},
eprint = {1611.02120},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smithson et al. - 2016 - Neural networks designing neural networks.pdf:pdf},
isbn = {9781450344661},
issn = {10923152},
month = {nov},
pages = {1--8},
title = {{Neural networks designing neural networks}},
OPTurl = {http://arxiv.org/abs/1611.02120 http://arxiv.org/abs/1611.02120{\%}0Ahttp://arxiv.org/abs/1611.02120{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2966986.2967058},
year = {2016}
}
@inproceedings{Xu2016,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
annote = {Theu suggest that ReLU is better than tanh and sigmoid and produce sparsity in the activations. Everything is true and is now widely accepted. However it does not benefit the project.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Xu, Lie and Choy, Chiu Sing and Li, Yi Wen},
booktitle = {2016 International Workshop on Acoustic Signal Enhancement, IWAENC 2016},
doi = {10.1109/IWAENC.2016.7602891},
eprint = {1502.03167},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Choy, Li - 2016 - Deep sparse rectifier neural networks for speech denoising.pdf:pdf},
isbn = {9781509020072},
issn = {15324435},
keywords = {Network pruning,Rectifier neurons,Sparseness,Speech denoising},
pages = {315--323},
title = {{Deep sparse rectifier neural networks for speech denoising}},
OPTurl = {https://www.utc.fr/{~}bordesan/dokuwiki/{\_}media/en/glorot10nipsworkshop.pdf},
volume = {15},
year = {2016}
}
@misc{Pan2010,
abstract = {A survey on transfer learning. IEEE Trans. Knowl. Data Eng},
author = {Pan, Sinno Jialin and Yang, Qiang},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2009.191},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Yang - 2010 - A survey on transfer learning.pdf:pdf},
isbn = {1041-4347 VO - 22},
issn = {10414347},
keywords = {Transfer learning,data mining.,machine learning,survey},
number = {10},
pages = {1345--1359},
title = {{A survey on transfer learning}},
volume = {22},
year = {2010}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the $\backslash$textit{\{}Information Plane{\}}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\{}$\backslash$emph compression{\}} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shwartz-Ziv, Tishby - 2017 - Opening the Black Box of Deep Neural Networks via Information.pdf:pdf},
journal = {arXiv},
keywords = {Deep Learning,Deep Neural Networks,Information Bottleneck,Representation Learning},
month = {mar},
pages = {1--19},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
OPTurl = {http://arxiv.org/abs/1703.00810 https://arxiv.org/pdf/1703.00810.pdf{\%}0Ahttp://arxiv.org/abs/1703.00810},
year = {2017}
}
@inproceedings{Hinton2011,
abstract = {The artificial neural networks that are used to recognise shapes typcially use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered representations of the pose of the feature, like SIFT, that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instatiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adpating the features to the domain},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Hinton, Geoffrey E and Krizhevsky, Alex and Wang, Sida D},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-21735-7_6},
eprint = {9605103},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Krizhevsky, Wang - 2011 - Transforming auto-encoders.pdf:pdf},
isbn = {9783642217340},
issn = {03029743},
keywords = {Invariance,auto-encoder,shape representation},
number = {PART 1},
pages = {44--51},
pmid = {1000183096},
primaryClass = {cs},
title = {{Transforming auto-encoders}},
OPTurl = {http://www.cs.toronto.edu/{~}fritz/absps/transauto6.pdf},
volume = {6791 LNCS},
year = {2011}
}
@article{Xiao2017,
abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
archivePrefix = {arXiv},
arxivId = {1708.07747},
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
eprint = {1708.07747},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao, Rasul, Vollgraf - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms.pdf:pdf},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
OPTurl = {https://arxiv.org/pdf/1708.07747.pdf http://arxiv.org/abs/1708.07747},
year = {2017}
}
@inproceedings{Abadi2016b,
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Brain, Google and Osdi, Implementation and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {OSDI},
eprint = {1605.08695},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow A System for Large-Scale Machine Learning.pdf:pdf},
isbn = {9781931971331},
title = {{TensorFlow : A System for Large-Scale Machine Learning}},
year = {2016}
}
@inproceedings{Liu2017,
abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.},
archivePrefix = {arXiv},
arxivId = {1708.06519},
author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
booktitle = {ICCV 2017},
eprint = {1708.06519},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2017 - Learning Efficient Convolutional Networks through Network Slimming.pdf:pdf},
month = {aug},
title = {{Learning Efficient Convolutional Networks through Network Slimming}},
OPTurl = {http://arxiv.org/abs/1708.06519},
year = {2017}
}
@article{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {{\ldots} Science Department, University of Toronto, Tech. {\ldots}},
pages = {1--60},
pmid = {25246403},
title = {{Learning Multiple Layers of Features from Tiny Images}},
OPTurl = {https://www.cs.toronto.edu/{~}kriz/learning-features-2009-TR.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Multiple+Layers+of+Features+from+Tiny+Images{\#}0},
year = {2009}
}
@article{Zoph2017a,
abstract = {Developing state-of-the-art image classification models often requires significant architecture engineering and tuning. In this paper, we attempt to reduce the amount of architecture engineering by using Neural Architecture Search to learn an architectural building block on a small dataset that can be transferred to a large dataset. This approach is similar to learning the structure of a recurrent cell within a recurrent network. In our experiments, we search for the best convolutional cell on the CIFAR-10 dataset and then apply this learned cell to the ImageNet dataset by stacking together more of this cell. Although the cell is not learned directly on ImageNet, an architecture constructed from the best learned cell achieves state-of-the-art accuracy of 82.3{\%} top-1 and 96.0{\%} top-5 on ImageNet, which is 0.8{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS. This cell can also be scaled down two orders of magnitude: a smaller network constructed from the best cell also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than the equivalently-sized, state-of-the-art models for mobile platforms.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
eprint = {1707.07012},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph et al. - 2017 - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
month = {jul},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
OPTurl = {http://arxiv.org/abs/1707.07012},
year = {2017}
}
@article{Bengio2012a,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2012 - Practical Recommendations for Gradient-Based Training of Deep Architectures.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {()},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
OPTurl = {https://arxiv.org/pdf/1206.5533v2.pdf},
volume = {7700 LECTU},
year = {2012}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
annote = {Motivation: Hard and long to design NN architectures

Insight: It is possible to represent networks as a flat sequence

Solution: Use reinforcement learning to predict the sequences

Takeaway: It is interesting to work on finding optimal size

Why this paper should not have been published:

I think there is a major flaw. They use recurent neural network which are made for processing input. But they actually don't have input. I could not find in the two papers related to this technique what is the input of their network. Is it noise, is it zero ? In any case I think it is just equalivatent to learn a compressed (maybe not even compressed) vector instead of a network. I would definitely compare against genetic or other algorithms. especially since they trained 12800 architectures. (this is insane !)},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V},
eprint = {1611.01578},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Topalis et al. - 2017 - Neural Architecture Search with Reinforcement Learning.pdf:pdf},
isbn = {9781424425051},
issn = {1938-7228},
title = {{Neural Architecture Search with Reinforcement Learning}},
OPTurl = {https://arxiv.org/pdf/1611.01578.pdf http://arxiv.org/abs/1611.01578},
year = {2016}
}
@article{Sabour,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.09829v1},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E and Toronto, Google Brain},
eprint = {arXiv:1710.09829v1},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sabour et al. - Unknown - Dynamic Routing Between Capsules.pdf:pdf},
title = {{Dynamic Routing Between Capsules}},
OPTurl = {https://arxiv.org/pdf/1710.09829.pdf}
}
@inproceedings{Zhao2015,
abstract = {The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to allow the representations of the same sentence to be engaged in a particular learning task (e.g., classification), therefore effectively mitigating the gradient vanishing problem persistent in other recursive models. Both qualitative and quantitative analysis shows that AdaSent can automatically form and select the representations suitable for the task at hand during training, yielding superior classification performance over competitor models on 5 benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Zhao, Han and Lu, Zhengdong and Poupart, Pascal},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
isbn = {9781577357384},
issn = {10450823},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {4069--4076},
pmid = {18244602},
title = {{Self-adaptive hierarchical sentence model}},
volume = {2015-Janua},
year = {2015}
}
@article{Han2015,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J},
doi = {abs/1510.00149/1510.00149},
eprint = {1510.00149},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Mao, Dally - Unknown - DEEP COMPRESSION COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING.pdf:pdf},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
OPTurl = {https://arxiv.org/pdf/1510.00149.pdf http://arxiv.org/abs/1510.00149},
year = {2015}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition(2).pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
OPTurl = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/He{\_}Deep{\_}Residual{\_}Learning{\_}CVPR{\_}2016{\_}paper.pdf http://ieeexplore.ieee.org/document/7780459/},
year = {2016}
}
@inproceedings{Han2016,
abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10{\^{}}4 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.},
archivePrefix = {arXiv},
arxivId = {1602.01528},
author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
booktitle = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
doi = {10.1109/ISCA.2016.30},
eprint = {1602.01528},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - Unknown - EIE Efficient Inference Engine on Compressed Deep Neural Network.pdf:pdf},
isbn = {9781467389471},
issn = {0163-5964},
keywords = {ASIC,Algorithm-Hardware co-Design,Deep Learning,Hardware Acceleration,Model Compression},
pages = {243--254},
pmid = {313940},
title = {{EIE: Efficient Inference Engine on Compressed Deep Neural Network}},
OPTurl = {https://arxiv.org/pdf/1602.01528.pdf http://arxiv.org/abs/1602.01528},
year = {2016}
}
@inproceedings{Mnih2010,
abstract = {Reliably extracting information fromaerial imagery is a difficult prob- lemwith many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detec- tion system is currently on the market and no published method has been shown to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The net- work is trained on massive amounts of data using a consumer GPU.We demon- strate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels.We show that our method works reliably on two challenging urban datasets that are an order ofmagnitude larger than what was used to evaluate previous approaches. 1},
author = {Mnih, Volodymyr and Hinton, Geoffrey E},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15567-3_16},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih, Hinton - 2010 - Learning to detect roads in high-resolution aerial images.pdf:pdf},
isbn = {3642155669},
issn = {03029743},
number = {PART 6},
pages = {210--223},
title = {{Learning to detect roads in high-resolution aerial images}},
volume = {6316 LNCS},
year = {2010}
}
@article{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Us-ing a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on ob-ject detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts de-tections for more than 9000 different object categories. And it still runs in real-time.},
author = {Redmon, Joseph},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon - 2017 - YOLO9000.pdf:pdf},
journal = {arxiv},
number = {April},
title = {{YOLO9000}},
OPTurl = {http://pjreddie.com/yolo9000/},
year = {2017}
}
@article{Pedregosa2016,
abstract = {Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.},
archivePrefix = {arXiv},
arxivId = {1602.02355},
author = {Pedregosa, Fabian},
eprint = {1602.02355},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedregosa - Unknown - Hyperparameter optimization with approximate gradient.pdf:pdf},
isbn = {9781510829008},
title = {{Hyperparameter optimization with approximate gradient}},
OPTurl = {https://arxiv.org/pdf/1602.02355.pdf http://arxiv.org/abs/1602.02355},
year = {2016}
}
@article{Cun,
abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cun, Yann Le and Denker, John S and Solla, Sara a},
doi = {10.1.1.32.7223},
eprint = {arXiv:1011.1669v3},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cun et al. - Unknown - Optimal Brain Damage.pdf:pdf},
isbn = {1558601007},
issn = {1098-6596},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {598--605},
pmid = {25246403},
title = {{Optimal Brain Damage}},
OPTurl = {https://papers.nips.cc/paper/250-optimal-brain-damage.pdf},
volume = {2},
year = {1990}
}
@inproceedings{Huang2016,
abstract = {Very deep convolutional networks with hundreds or more layers have lead to significant reductions in error on competitive benchmarks like the ImageNet or COCO tasks. Although the unmatched expressiveness of the many deep layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes and the training time can be painfully slow even on modern computers. In this paper we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and obtain deep networks. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. The resulting networks are short (in expectation) during training and deep during testing. Training Residual Networks with stochastic depth is compellingly simple to implement, yet effective. We show that this approach successfully addresses the training difficulties of deep networks and complements the recent success of Residual and Highway Networks. It reduces training time substantially and improves the test errors on almost all data sets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, we show that with stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91{\%}) on CIFAR-10.},
archivePrefix = {arXiv},
arxivId = {1603.09382},
author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_39},
eprint = {1603.09382},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2016 - Deep networks with stochastic depth.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
month = {mar},
pages = {646--661},
pmid = {4520227},
title = {{Deep networks with stochastic depth}},
OPTurl = {http://arxiv.org/abs/1603.09382},
volume = {9908 LNCS},
year = {2016}
}
@inproceedings{Philipp,
abstract = {Automatically determining the optimal size of a neural network for a given task without requiring input from the user currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of finding a good network size during a single training cycle. We introduce nonpara- metric neural networks, a non-probabilistic framework for conducting optimiza- tion over all possible network sizes. We prove its soundness when network growth is limited via an ?p penalty. We train networks under this framework by continu- ously adding new units and employing a novel optimization algorithm, which we term “Adaptive Radial-Angular Gradient Descent” or AdaRad. We obtain promis- ing results.},
archivePrefix = {arXiv},
arxivId = {1712.05440},
author = {Philipp, George and Carbonell, Jaime G},
booktitle = {Proc. International Conference on Learning Representations},
eprint = {1712.05440},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Philipp, Carbonell - Unknown - NONPARAMETRIC NEURAL NETWORKS.pdf:pdf},
number = {2016},
pages = {1--27},
title = {{Nonparametric Neural Network}},
OPTurl = {https://www.cs.cmu.edu/{~}jgc/publication/Nonparametric Neural Networks.pdf https://openreview.net/pdf?id=BJK3Xasel},
year = {2017}
}
@article{Roche2005,
abstract = {Corporate learning programs are under pressure to show a better return on investment, even as evidence mounts that classroom training may not be enough to ensure transfer of learning to the job. To improve its bottom-line impact, the Global Learning and Leadership Development group at Agilent Technologies stepped outside the “classroom box” to focus on the learning period after formal instruction ends. It implemented a new follow-through system and other innovations to encourage on-the-job application of course learning and to facilitate coaching and feedback, particularly from participants'managers, who can make the difference in successful learning transfer. {\textcopyright} 2005 Wiley Periodicals, Inc. [ABSTRACT FROM AUTHOR]},
archivePrefix = {arXiv},
arxivId = {1701.06106},
author = {Roche, Teresa and Wick, Cal and Stewart, Marie},
doi = {10.1002/joe.20070},
eprint = {1701.06106},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roche, Wick, Stewart - 2005 - Innovation in learning Agilent technologies thinks outside the box.pdf:pdf},
isbn = {15311864},
issn = {15311864},
journal = {Journal of Organizational Excellence},
month = {oct},
number = {4},
pages = {45},
title = {{Innovation in learning: Agilent technologies thinks outside the box}},
OPTurl = {http://arxiv.org/abs/1710.10196},
volume = {24},
year = {2005}
}
@article{Bach2011,
abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. They were first dedicated to linear variable selection but numerous extensions have now emerged such as structured sparsity or kernel selection. It turns out that many of the related estimation problems can be cast as convex optimization problems by regularizing the empirical risk with appropriate non-smooth norms. The goal of this paper is to present from a general perspective optimization tools and techniques dedicated to such sparsity-inducing penalties. We cover proximal methods, block-coordinate descent, reweighted {\$}\backslashell{\_}2{\$}-penalized techniques, working-set and homotopy methods, as well as non-convex formulations and extensions, and provide an extensive set of experiments to compare various algorithms from a computational point of view.},
archivePrefix = {arXiv},
arxivId = {1108.0775},
author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
doi = {10.1561/2200000015},
eprint = {1108.0775},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2011 - Optimization with Sparsity-Inducing Penalties.pdf:pdf},
isbn = {9780262016469},
issn = {1935-8237},
keywords = {()},
title = {{Optimization with Sparsity-Inducing Penalties}},
OPTurl = {https://arxiv.org/pdf/1108.0775.pdf http://arxiv.org/abs/1108.0775},
year = {2011}
}
@article{Raghu2017,
abstract = {With the continuing empirical successes of deep networks, it becomes increasingly important to develop better methods for understanding training of models and the representations learned within. In this paper we propose Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
archivePrefix = {arXiv},
arxivId = {1706.05806},
author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
eprint = {1706.05806},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raghu et al. - 2017 - SVCCA Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement.pdf:pdf},
month = {jun},
title = {{SVCCA: Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement}},
OPTurl = {http://arxiv.org/abs/1706.05806},
year = {2017}
}
@misc{Tibshirani1996,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society B},
doi = {10.2307/2346178},
eprint = {11/73273},
isbn = {0849320240},
issn = {00359246},
number = {1},
pages = {267--288},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression Selection and Shrinkage via the Lasso}},
OPTurl = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7574},
volume = {58},
year = {1996}
}
@inproceedings{Topalis2017,
abstract = {Neural networks are powerful and flexible models that work well for many diffi- cult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a re- current network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of- the-art.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Topalis, E and Prayati, A and Antonopoulos, C and Koubias, S and Campus, Rio},
booktitle = {ICLR},
eprint = {1611.01578},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Topalis et al. - 2017 - Neural Architecture Search with Reinforcement Learning.pdf:pdf},
isbn = {9781424425051},
month = {nov},
pages = {976--981},
title = {{Neural Architecture Search with Reinforcement Learning}},
OPTurl = {http://arxiv.org/abs/1611.01578},
year = {2017}
}
@inproceedings{Jouppi2017,
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a ​ Tensor Processing Unit (TPU) ​ — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, {\ldots}) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95{\%} of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU. Index terms–DNN, MLP, CNN, RNN, LSTM, neural network, domain-specific architecture, accelerator},
archivePrefix = {arXiv},
arxivId = {1704.04760},
author = {Jouppi, Norman P. and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Young, Cliff and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Patil, Nishant and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Patterson, David and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Agrawal, Gaurav and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Bajwa, Raminder and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Bates, Sarah and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun and Bhatia, Suresh and Boden, Nan},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture - ISCA '17},
doi = {10.1145/3079856.3080246},
eprint = {1704.04760},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Processing Unit.pdf:pdf},
isbn = {9781450348928},
issn = {10636897},
month = {apr},
pages = {1--12},
title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
OPTurl = {http://arxiv.org/abs/1704.04760 http://dl.acm.org/citation.cfm?doid=3079856.3080246},
year = {2017}
}
@article{Scardapane2017,
abstract = {In this paper, we address the challenging task of simultaneously optimizing (i) the weights of a neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are traditionally dealt with separately, we propose an efficient regularized formulation enabling their simultaneous parallel execution, using standard optimization routines. Specifically, we extend the group Lasso penalty, originally proposed in the linear regression literature, to impose group-level sparsity on the network's connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We carry out an extensive experimental evaluation, in comparison with classical weight decay and Lasso penalties, both on a toy dataset for handwritten digit recognition, and multiple realistic mid-scale classification benchmarks. Comparative results demonstrate the potential of our proposed sparse group Lasso penalty in producing extremely compact networks, with a significantly lower number of input features, with a classification accuracy which is equal or only slightly inferior to standard regularization terms.},
archivePrefix = {arXiv},
arxivId = {1607.00485},
author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
doi = {10.1016/j.neucom.2017.02.029},
eprint = {1607.00485},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scardapane et al. - 2017 - Group sparse regularization for deep neural networks.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep networks,Feature selection,Group sparsity,Pruning},
pages = {81--89},
title = {{Group sparse regularization for deep neural networks}},
OPTurl = {https://arxiv.org/pdf/1607.00485.pdf},
volume = {241},
year = {2017}
}
@article{Cardot2015,
abstract = {In the current context of data explosion, online techniques that do not require storing all data in memory are indispensable to routinely perform tasks like principal component analysis (PCA). Recursive algorithms that update the PCA with each new observation have been studied in various fields of research and found wide applications in industrial monitoring, computer vision, astronomy, and latent semantic indexing, among others. This work provides guidance for selecting an online PCA algorithm in practice. We present the main approaches to online PCA, namely, perturbation techniques, incremental methods, and stochastic optimization, and compare their statistical accuracy, computation time, and memory requirements using artificial and real data. Extensions to missing data and to functional data are discussed. All studied algorithms are available in the R package onlinePCA on CRAN.},
archivePrefix = {arXiv},
arxivId = {1511.03688},
author = {Cardot, Herv{\'{e}} and Degras, David},
doi = {10.1111/insr.12220},
eprint = {1511.03688},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cardot, Degras - 2015 - Online Principal Component Analysis in High Dimension Which Algorithm to Choose.pdf:pdf},
issn = {17515823},
keywords = {Covariance matrix,Eigendecomposition,Generalized hebbian algorithm,Incremental SVD,Perturbation methods,Recursive algorithms,Stochastic gradient},
title = {{Online Principal Component Analysis in High Dimension: Which Algorithm to Choose?}},
OPTurl = {https://arxiv.org/pdf/1511.03688.pdf http://arxiv.org/abs/1511.03688},
year = {2015}
}
@article{Kwak2002,
abstract = {Feature selection plays an important role in classifying systems such as neural networks (NNs). We use a set of attributes which are relevant, irrelevant or redundant and from the viewpoint of managing a dataset which can be huge, reducing the number of attributes by selecting only the relevant ones is desirable. In doing so, higher performances with lower computational effort is expected. In this paper, we propose two feature selection algorithms. The limitation of mutual information feature selector (MIFS) is analyzed and a method to overcome this limitation is studied. One of the proposed algorithms makes more considered use of mutual information between input attributes and output classes than the MIFS. What is demonstrated is that the proposed method can provide the performance of the ideal greedy selection algorithm when information is distributed uniformly. The computational load for this algorithm is nearly the same as that of MIFS. In addition, another feature selection algorithm using the Taguchi method is proposed. This is advanced as a solution to the question as to how to identify good features with as few experiments as possible. The proposed algorithms are applied to several classification problems and compared with MIFS. These two algorithms can be combined to complement each other's limitations. The combined algorithm performed well in several experiments and should prove to be a useful method in selecting features for classification problems.},
author = {Kwak, Nojun and Choi, Chong-Ho},
doi = {10.1109/72.977291},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwak, Choi - 2002 - Input feature selection for classification problems.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
number = {1},
pages = {143--159},
pmid = {18244416},
title = {{Input feature selection for classification problems.}},
volume = {13},
year = {2002}
}
@inproceedings{Bergstra2011,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap-proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos-sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu-ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex-pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli-able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Bergstra, James S and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
booktitle = {Advances in Neural Information Processing Systems},
doi = {2012arXiv1206.2944S},
eprint = {1206.2944},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra et al. - 2011 - Algorithms for hyper-parameter optimization.pdf:pdf},
isbn = {9781618395993},
issn = {10495258},
pages = {2546--2554},
pmid = {9377276},
title = {{Algorithms for hyper-parameter optimization}},
OPTurl = {https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
year = {2011}
}
@inproceedings{Sainath2013,
abstract = {While Deep Neural Networks (DNNs) have achieved tremendous success for large vocabulary continuous speech recognition (LVCSR) tasks, training of these networks is slow. One reason is that DNNs are trained with a large number of training parameters (i.e., 10-50 million). Because networks are trained with a large number of output targets to achieve good performance, the majority of these parameters are in the final weight layer. In this paper, we propose a low-rank matrix factorization of the final weight layer. We apply this low-rank technique to DNNs for both acoustic modeling and language modeling. We show on three different LVCSR tasks ranging between 50-400 hrs, that a low-rank factorization reduces the number of parameters of the network by 30-50{\%}. This results in roughly an equivalent reduction in training time, without a significant loss in final recognition accuracy, compared to a full-rank representation.},
author = {Sainath, Tara N. and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6638949},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sainath et al. - 2013 - Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
pages = {6655--6659},
title = {{Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets}},
OPTurl = {http://ieeexplore.ieee.org/document/6638949/},
year = {2013}
}
@inproceedings{Ng2004,
abstract = {We consider supervised learning in the pres-ence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logis-tic regression, we show that using L 1 regu-larization of the parameters, the sample com-plexity (i.e., the number of training examples required to learn " well, ") grows only loga-rithmically in the number of irrelevant fea-tures. This logarithmic rate matches the best known bounds for feature selection, and in-dicates that L 1 regularized logistic regression can be effective even if there are exponen-tially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invari-ant algorithm—including logistic regression with L 2 regularization, SVMs, and neural networks trained by backpropagation—has a worst case sample complexity that grows at least linearly in the number of irrelevant fea-tures.},
author = {Ng, Andrew Y},
booktitle = {ICML},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - Unknown - Feature selection, L1 vs. L2 regularization, and rotational invariance.pdf:pdf},
pages = {78--85},
title = {{Feature selection, $\backslash$ell{\_}1 vs. $\backslash$ell{\_}2 regularization, and rotational invariance.}},
OPTurl = {http://www.machinelearning.org/proceedings/icml2004/papers/354.pdf},
year = {2004}
}
@article{Klambauer2017,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
doi = {1706.02515},
eprint = {1706.02515},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:pdf},
title = {{Self-Normalizing Neural Networks}},
OPTurl = {https://arxiv.org/pdf/1706.02515.pdf http://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{Ravi2017,
abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTM- based meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametriza- tion of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick con- vergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
author = {Ravi, Sachin and Larochelle, Hugo},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravi, Larochelle - 2017 - Optimization As a Model for Few-Shot Learning.pdf:pdf},
journal = {ICLR},
pages = {1--11},
title = {{Optimization As a Model for Few-Shot Learning}},
OPTurl = {https://openreview.net/pdf?id=rJY0-Kcll},
year = {2017}
}
@article{Zoph2017,
abstract = {Developing state-of-the-art image classification models often requires significant architecture engineering and tuning. In this paper, we attempt to reduce the amount of architecture engineering by using Neural Architecture Search to learn an architectural building block on a small dataset that can be transferred to a large dataset. This approach is similar to learning the structure of a recurrent cell within a recurrent network. In our experiments, we search for the best convolutional cell on the CIFAR-10 dataset and then apply this learned cell to the ImageNet dataset by stacking together more of this cell. Although the cell is not learned directly on ImageNet, an architecture constructed from the best learned cell achieves state-of-the-art accuracy of 82.3{\%} top-1 and 96.0{\%} top-5 on ImageNet, which is 0.8{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS. This cell can also be scaled down two orders of magnitude: a smaller network constructed from the best cell also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than the equivalently-sized, state-of-the-art models for mobile platforms.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
eprint = {1707.07012},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph et al. - 2017 - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
month = {jul},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
OPTurl = {http://arxiv.org/abs/1707.07012},
year = {2017}
}
@article{Hosang2016,
abstract = {Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL and ImageNet, and impact on DPM and R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detector performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.},
archivePrefix = {arXiv},
arxivId = {1502.05082},
author = {Hosang, Jan and Benenson, Rodrigo and Dollar, Piotr and Schiele, Bernt},
doi = {10.1109/TPAMI.2015.2465908},
eprint = {1502.05082},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosang et al. - 2016 - What Makes for Effective Detection Proposals.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer Vision,Detection proposals,object detection},
number = {4},
pages = {814--830},
pmid = {26959679},
title = {{What Makes for Effective Detection Proposals?}},
volume = {38},
year = {2016}
}
@article{Suzuki,
abstract = {This paper describes an approach to synthesizing desired ¢lters using a multilayer neural network (NN). In order to acquire the right function of the object ¢lter, a simple method for reducing the structures of both the input and the hidden layers of the NN is proposed. In the proposed method, the units are removed from the NN on the basis of the in{\pounds}uence of removing each unit on the error, and the NN is retrained to recover the damage of the removal. Each process is performed alternately, and then the structure is reduced. Experiments to synthesize a known ¢lter were performed. By the analysis of the NN obtained by the proposed method, it has been shown that it acquires the right function of the object ¢lter. By the experiment to synthesize the ¢lter for solving real signal processing tasks, it has been shown that the NN obtained by the proposed method is superior to that obtained by the conventional method in terms of the ¢lter performance and the computational cost.},
annote = {Suggest that we should evalute the impact on the loss of each neuron. Which is clearly too expensive on modern networks},
author = {Suzuki, Kenji and Horiba, Isao and Sugie, Noboru},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suzuki, Horiba, Sugie - Unknown - A Simple Neural Network Pruning Algorithm with Application to Filter Synthesis.pdf:pdf},
keywords = {generalization ability,image enhancement,neural ¢lter,optimal structure,redundancy removal,right function,signal processing},
title = {{A Simple Neural Network Pruning Algorithm with Application to Filter Synthesis}},
OPTurl = {https://link.springer.com/content/pdf/10.1023/A:1009639214138.pdf}
}
@article{Louizos2017,
abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by taking a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1705.08665},
author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
eprint = {1705.08665},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Louizos, Ullrich, Welling - 2017 - Bayesian Compression for Deep Learning.pdf:pdf},
month = {may},
title = {{Bayesian Compression for Deep Learning}},
OPTurl = {http://arxiv.org/abs/1705.08665},
year = {2017}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
doi = {10.1051/0004-6361/201527329},
eprint = {1603.07285},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Arxiv},
pages = {1--28},
pmid = {26353135},
title = {{A guide to convolution arithmetic for deep learning}},
OPTurl = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@inproceedings{Lecun1998,
abstract = {| Multilayer Neural Networks trained with the backpropa-gation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten char-acters, with minimal preprocessing. This paper reviews var-ious methods applied to handwritten character recognition and compares them on a standard handwritten digit recog-nition task. Convolutional Neural Networks, that are specif-ically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including eld extraction, segmenta-tion, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), al-lows such m ulti-module systems to be trained globally using Gradient-Based methods so as to minimize an overall per-formance measure. Two systems for on-line handwriting recognition are de-scribed. Experiments demonstrate the advantage of global training, and the eexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check i s also described. It uses Convolutional Neural Network char-acter recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks perday.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Y and Bottou, L and Bengio, Yoshua and Haffner, P},
booktitle = {Intelligent Signal Processing},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun et al. - 1998 - Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
pages = {306--351},
pmid = {15823584},
title = {{Gradient-Based Learning Applied to Document Recognition}},
OPTurl = {http://vision.stanford.edu/cs598{\_}spring07/papers/Lecun98.pdf},
year = {2001}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
volume = {7700 LECTU},
year = {2012}
}
@article{Keskar2014,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32–512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generaliza-tion drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions—and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat min-imizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tak, Ping and Tang, Peter},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {1609.04836},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keskar et al. - 2014 - ON LARGE-BATCH TRAINING FOR DEEP LEARNING GENERALIZATION GAP AND SHARP MINIMA.pdf:pdf},
isbn = {9781405161251},
issn = {1607-551X},
number = {2},
pages = {1--134},
pmid = {17460516},
title = {{ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA}},
OPTurl = {https://arxiv.org/pdf/1609.04836.pdf},
year = {2014}
}
@article{Huang2016a,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74{\%} test error on CIFAR-10, 19.25{\%} on CIFAR-100 and 1.59{\%} on SVHN).},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q},
eprint = {1608.06993},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Liu, Weinberger - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
journal = {arXiv preprint},
pages = {1--12},
pmid = {211888},
title = {{Densely Connected Convolutional Networks}},
OPTurl = {http://arxiv.org/abs/1608.06993},
year = {2016}
}
@article{DBLP:journals/corr/IoffeS15,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/1502.03167},
  year      = {2015},
  OPTurl       = {http://arxiv.org/abs/1502.03167},
  archivePrefix = {arXiv},
  eprint    = {1502.03167},
  timestamp = {Wed, 07 Jun 2017 14:40:49 +0200},
  bibOPTurl    = {http://dblp.org/rec/bib/journals/corr/IoffeS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
OPTurl = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}
@article{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  OPTurl       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
  bibOPTurl    = {http://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{BergstraJAMESBERGSTRA2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Bergstra, James and Yoshua, Bengio},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra JAMESBERGSTRA, Yoshua Bengio YOSHUABENGIO - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
pmid = {18244602},
title = {{Random Search for Hyper-Parameter Optimization}},
OPTurl = {http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nair, Hinton - Unknown - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
OPTurl = {http://www.cs.toronto.edu/{~}fritz/absps/reluICML.pdf},
year = {2010}
}

@article{Aghasi2016,
abstract = {We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program. This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model. The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm. While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner. In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model. To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length {\$}N{\$} as inputs, we show that if the network response can be described using a maximum number of {\$}s{\$} non-zero weights per node, these weights can be learned from {\$}\backslashmathcal{\{}O{\}}(s\backslashlog N){\$} samples.},
archivePrefix = {arXiv},
arxivId = {1611.05162},
author = {Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},
eprint = {1611.05162},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aghasi et al. - Unknown - Net-Trim Convex Pruning of Deep Neural Networks with Performance Guarantee.pdf:pdf},
title = {{Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee}},
OPTurl = {https://papers.nips.cc/paper/6910-net-trim-convex-pruning-of-deep-neural-networks-with-performance-guarantee.pdf http://arxiv.org/abs/1611.05162},
year = {2016}
}

@article{Yuan2006,
abstract = {Summary. We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
author = {Yuan, Ming and Lin, Yi},
doi = {10.1111/j.1467-9868.2005.00532.x},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan, Lin - 2006 - Model selection and estimation in regression with grouped variables.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Analysis of variance,Lasso,Least angle regression,Non-negative garrotte,Piecewise linear solution path},
number = {1},
pages = {49--67},
pmid = {11161800},
title = {{Model selection and estimation in regression with grouped variables}},
OPTurl = {http://pages.stat.wisc.edu/{~}myuan/papers/glasso.final.pdf},
volume = {68},
year = {2006}
}


