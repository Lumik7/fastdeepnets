Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Liu,
abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.},
archivePrefix = {arXiv},
arxivId = {1708.06519},
author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
doi = {10.1109/ICCV.2017.298},
eprint = {1708.06519},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - Unknown - Learning Efficient Convolutional Networks through Network Slimming.pdf:pdf},
isbn = {978-1-5386-1032-9},
issn = {15505499},
journal = {ICCV 2017},
title = {{Learning Efficient Convolutional Networks through Network Slimming}},
url = {https://arxiv.org/pdf/1708.06519.pdf http://arxiv.org/abs/1708.06519},
year = {2017}
}
@article{Zoph2017a,
abstract = {Developing state-of-the-art image classification models often requires significant architecture engineering and tuning. In this paper, we attempt to reduce the amount of architecture engineering by using Neural Architecture Search to learn an architectural building block on a small dataset that can be transferred to a large dataset. This approach is similar to learning the structure of a recurrent cell within a recurrent network. In our experiments, we search for the best convolutional cell on the CIFAR-10 dataset and then apply this learned cell to the ImageNet dataset by stacking together more of this cell. Although the cell is not learned directly on ImageNet, an architecture constructed from the best learned cell achieves state-of-the-art accuracy of 82.3{\%} top-1 and 96.0{\%} top-5 on ImageNet, which is 0.8{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS. This cell can also be scaled down two orders of magnitude: a smaller network constructed from the best cell also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than the equivalently-sized, state-of-the-art models for mobile platforms.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
eprint = {1707.07012},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph et al. - 2017 - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
month = {jul},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
url = {http://arxiv.org/abs/1707.07012},
year = {2017}
}
@article{Arora2017,
abstract = {Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of (Goodfellow et al 2014) suggested they do, if they were given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al (to appear at ICML 2017) raised doubts whether the same holds when discriminator has finite size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support ---in other words, the training objective is unable to prevent mode collapse. The current note reports experiments suggesting that such problems are not merely theoretical. It presents empirical evidence that well-known GANs approaches do learn distributions of fairly low support, and thus presumably are not learning the target distribution. The main technical contribution is a new proposed test, based upon the famous birthday paradox, for estimating the support size of the generated distribution.},
archivePrefix = {arXiv},
arxivId = {1706.08224},
author = {Arora, Sanjeev and Zhang, Yi},
eprint = {1706.08224},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Zhang - 2017 - Do GANs actually learn the distribution An empirical study.pdf:pdf},
month = {jun},
title = {{Do GANs actually learn the distribution? An empirical study}},
url = {http://arxiv.org/abs/1706.08224},
year = {2017}
}
@article{Xie2016,
abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
archivePrefix = {arXiv},
arxivId = {1611.05431},
author = {Xie, Saining and Girshick, Ross and Doll{\'{a}}r, Piotr and Tu, Zhuowen and He, Kaiming},
doi = {10.1109/CVPR.2017.634},
eprint = {1611.05431},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2016 - Aggregated Residual Transformations for Deep Neural Networks.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {nov},
title = {{Aggregated Residual Transformations for Deep Neural Networks}},
url = {http://arxiv.org/abs/1611.05431},
year = {2016}
}
@article{Bach2011,
abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. They were first dedicated to linear variable selection but numerous extensions have now emerged such as structured sparsity or kernel selection. It turns out that many of the related estimation problems can be cast as convex optimization problems by regularizing the empirical risk with appropriate non-smooth norms. The goal of this paper is to present from a general perspective optimization tools and techniques dedicated to such sparsity-inducing penalties. We cover proximal methods, block-coordinate descent, reweighted {\$}\backslashell{\_}2{\$}-penalized techniques, working-set and homotopy methods, as well as non-convex formulations and extensions, and provide an extensive set of experiments to compare various algorithms from a computational point of view.},
archivePrefix = {arXiv},
arxivId = {1108.0775},
author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
doi = {10.1561/2200000015},
eprint = {1108.0775},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2011 - Optimization with Sparsity-Inducing Penalties.pdf:pdf},
isbn = {9780262016469},
issn = {1935-8237},
keywords = {()},
title = {{Optimization with Sparsity-Inducing Penalties}},
url = {https://arxiv.org/pdf/1108.0775.pdf http://arxiv.org/abs/1108.0775},
year = {2011}
}
@article{Xiao2017,
abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
archivePrefix = {arXiv},
arxivId = {1708.07747},
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
eprint = {1708.07747},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao, Rasul, Vollgraf - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms.pdf:pdf},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
url = {https://arxiv.org/pdf/1708.07747.pdf http://arxiv.org/abs/1708.07747},
year = {2017}
}
@article{Kingma2015a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
isbn = {9781450300728},
issn = {09252312},
journal = {International Conference on Learning Representations 2015},
pages = {1--15},
pmid = {172668},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parame-ters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phe-nomenon as internal covariate shift, and ad-dress the problem by normalizing layer inputs. Our method draws its strength from making nor-malization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less care-ful about initialization, and in some cases elim-inates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Nor-malization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensem-ble of batch-normalized networks, we improve upon the best published result on ImageNet clas-sification: reaching 4.82{\%} top-5 test error, ex-ceeding the accuracy of human raters.},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {ICML},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
pages = {448--456},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://proceedings.mlr.press/v37/ioffe15.pdf},
year = {2015}
}
@incollection{Bottou2012,
abstract = {Validation can be used to detect when over tting starts during supervised training of a neural network; training is then stopped before convergence to avoid the over tting ($\backslash$early stopping"). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeo between training time and generalization: From the given mix of 1296 training runs using di erent 12 problems and 24 di erent network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4{\%} on average), but cost much more training time (here: about factor 4 longer on average).},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Bottou, L{\'{e}}on},
booktitle = {Neural Networks: Tricks of the Trade},
doi = {10.1007/978-3-642-35289-8},
eprint = {9780201398298},
isbn = {978-3-642-35288-1},
issn = {0302-9743},
number = {1},
pages = {421--436},
pmid = {25382349},
title = {{Neural Networks: Tricks of the Trade}},
url = {http://link.springer.com/10.1007/978-3-642-35289-8},
volume = {7700},
year = {2012}
}
@article{Koller1996,
abstract = {In this paper? w e examine a method for feature subset selection based on Information Theory ? Initially ? a framew ork for de?ning the theoretically optimal? but computationally in? tractable? method for feature subset selection is presen ted? W e sho w that our goal should be to eliminate a feature if it giv es us little or no additional information bey ond that subsumed b y the remaining features? In particular? this will be the case for both irrelev an t and redundan t features? W e then giv e an e?cien t algorithm for feature selection whic h computes an appro xi? mation to the optimal feature selection criterion? The conditions under whic h the appro ximate algorithm is successful are examined? Empirical results are giv en on a n um ber of data sets? sho wing that the algorithm e?ectiv ely handles datasets with a v ery large n um ber of features?},
author = {Koller, Daphne and Sahami, Mehran},
doi = {citeulike-article-id:393144},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koller, Sahami - 1996 - Toward optimal feature selection.pdf:pdf},
isbn = {0006-3495 (Print) 0006-3495 (Linking)},
issn = {0018-9286},
journal = {International Conference on Machine Learning (1996)},
keywords = {entropy,feature selection},
pages = {284--292},
pmid = {8804598},
title = {{Toward optimal feature selection}},
url = {https://ai.stanford.edu/{~}koller/Papers/Koller+Sahami:ICML96.pdf},
year = {1996}
}
@article{NVIDIA2017,
abstract = {Since the introduction of the pioneering CUDA GPU Computing platform over 10 years ago, each new NVIDIA{\textregistered} GPU generation has delivered higher application performance, improved power efficiency, added important new compute features, and simplified GPU programming. Today, NVIDIA GPUs accelerate thousands of High Performance Computing (HPC), data center, and machine learning applications. NVIDIA GPUs have become the leading computational engines powering the Artificial Intelligence (AI) revolution. NVIDIA GPUs accelerate numerous deep learning systems and applications including autonomous vehicle platforms, high-accuracy speech, image, and text recognition systems, intelligent video analytics, molecular simulations, drug discovery, disease diagnosis, weather forecasting, big data analytics, financial modeling, robotics, factory automation, real-time language translation, online search optimizations, and personalized user recommendations, to name just a few. The new NVIDIA{\textregistered} Tesla{\textregistered} V100 accelerator (shown in Figure 1) incorporates the powerful new Volta™ GV100 GPU. GV100 not only builds upon the advances of its predecessor, the Pascal™ GP100 GPU, it significantly improves performance and scalability, and adds many new features that improve programmability. These advances will supercharge HPC, data center, supercomputer, and deep learning systems and applications. This white paper presents the Tesla V100 accelerator and the Volta GV100 GPU architecture.},
author = {Nvidia},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nvidia - 2017 - Nvidia Tesla V100 Gpu Architecture.pdf:pdf},
keywords = {Volta},
number = {v1.1},
pages = {53},
title = {{Nvidia Tesla V100 Gpu Architecture}},
url = {http://composter.com.ua/documents/Volta-Architecture-Whitepaper.pdf https://images.nvidia.com/content/volta-architecture/pdf/Volta-Architecture-Whitepaper-v1.0.pdf http://www.nvidia.com/content/gated-pdfs/Volta-Architecture-Whitepaper-v1.1.pdf},
year = {2017}
}
@inproceedings{Zhao2015,
abstract = {The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to allow the representations of the same sentence to be engaged in a particular learning task (e.g., classification), therefore effectively mitigating the gradient vanishing problem persistent in other recursive models. Both qualitative and quantitative analysis shows that AdaSent can automatically form and select the representations suitable for the task at hand during training, yielding superior classification performance over competitor models on 5 benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Zhao, Han and Lu, Zhengdong and Poupart, Pascal},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
isbn = {9781577357384},
issn = {10450823},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {4069--4076},
pmid = {18244602},
title = {{Self-adaptive hierarchical sentence model}},
volume = {2015-Janua},
year = {2015}
}
@article{Loshchilov2017,
abstract = {We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW},
archivePrefix = {arXiv},
arxivId = {1711.05101},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1711.05101},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loshchilov, Hutter - 2017 - Fixing Weight Decay Regularization in Adam.pdf:pdf},
month = {nov},
title = {{Fixing Weight Decay Regularization in Adam}},
url = {https://arxiv.org/abs/1711.05101 http://arxiv.org/abs/1711.05101},
year = {2017}
}
@article{Jastrzebski2017,
abstract = {We study the properties of the endpoint of stochastic gradient descent (SGD). By approximating SGD as a stochastic differential equation (SDE) we consider the Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption of isotropic variance in loss gradients. Through this analysis, we find that three factors - learning rate, batch size and the variance of the loss gradients - control the trade-off between the depth and width of the minima found by SGD, with wider minima favoured by a higher ratio of learning rate to batch size. We have direct control over the learning rate and batch size, while the variance is determined by the choice of model architecture, model parameterization and dataset. In the equilibrium distribution only the ratio of learning rate to batch size appears, implying that the equilibrium distribution is invariant under a simultaneous rescaling of learning rate and batch size by the same amount. We then explore experimentally how learning rate and batch size affect SGD from two perspectives: the endpoint of SGD and the dynamics that lead up to it. For the endpoint, the experiments suggest the endpoint of SGD is invariant under simultaneous rescaling of batch size and learning rate, and also that a higher ratio leads to flatter minima, both findings are consistent with our theoretical analysis. We note experimentally that the dynamics also seem to be invariant under the same rescaling of learning rate and batch size, which we explore showing that one can exchange batch size and learning rate for cyclical learning rate schedule. Next, we illustrate how noise affects memorization, showing that high noise levels lead to better generalization. Finally, we find experimentally that the invariance under simultaneous rescaling of learning rate and batch size breaks down if the learning rate gets too large or the batch size gets too small.},
archivePrefix = {arXiv},
arxivId = {1711.04623},
author = {Jastrz{\c{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
eprint = {1711.04623},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jastrz{\c{e}}bski et al. - 2017 - Three Factors Influencing Minima in SGD.pdf:pdf},
month = {nov},
title = {{Three Factors Influencing Minima in SGD}},
url = {http://arxiv.org/abs/1711.04623},
year = {2017}
}
@article{Toderici,
abstract = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3{\%}-8.8{\%} AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
archivePrefix = {arXiv},
arxivId = {1608.05148},
author = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
doi = {10.4135/9781412985277},
eprint = {1608.05148},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Toderici et al. - Unknown - Full Resolution Image Compression with Recurrent Neural Networks.pdf:pdf},
isbn = {9780761914402},
issn = {08936080},
journal = {arXiv:1608.05148},
pages = {59},
pmid = {21655600},
title = {{Full Resolution Image Compression with Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1608.05148.pdf http://arxiv.org/abs/1608.05148},
year = {2016}
}
@article{Bengio2012a,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 2012 - Practical recommendations for gradient-based training of deep architectures.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {()},
month = {jun},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
url = {https://arxiv.org/pdf/1206.5533v2.pdf http://arxiv.org/abs/1206.5533},
volume = {7700 LECTU},
year = {2012}
}
@article{BergstraJAMESBERGSTRA2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Bergstra, James and Yoshua, Bengio},
doi = {10.1162/153244303322533223},
eprint = {1504.05070},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra, Yoshua - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
pmid = {18244602},
title = {{Random Search for Hyper-Parameter Optimization}},
url = {http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@article{Suzuki,
abstract = {This paper describes an approach to synthesizing desired ¢lters using a multilayer neural network (NN). In order to acquire the right function of the object ¢lter, a simple method for reducing the structures of both the input and the hidden layers of the NN is proposed. In the proposed method, the units are removed from the NN on the basis of the in{\pounds}uence of removing each unit on the error, and the NN is retrained to recover the damage of the removal. Each process is performed alternately, and then the structure is reduced. Experiments to synthesize a known ¢lter were performed. By the analysis of the NN obtained by the proposed method, it has been shown that it acquires the right function of the object ¢lter. By the experiment to synthesize the ¢lter for solving real signal processing tasks, it has been shown that the NN obtained by the proposed method is superior to that obtained by the conventional method in terms of the ¢lter performance and the computational cost.},
annote = {Suggest that we should evalute the impact on the loss of each neuron. Which is clearly too expensive on modern networks},
author = {Suzuki, Kenji and Horiba, Isao and Sugie, Noboru},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suzuki, Horiba, Sugie - Unknown - A Simple Neural Network Pruning Algorithm with Application to Filter Synthesis.pdf:pdf},
keywords = {generalization ability,image enhancement,neural ¢lter,optimal structure,redundancy removal,right function,signal processing},
title = {{A Simple Neural Network Pruning Algorithm with Application to Filter Synthesis}},
url = {https://link.springer.com/content/pdf/10.1023/A:1009639214138.pdf}
}
@inproceedings{Abadi2016b,
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Brain, Google and Osdi, Implementation and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {OSDI},
eprint = {1605.08695},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow A System for Large-Scale Machine Learning.pdf:pdf},
isbn = {9781931971331},
title = {{TensorFlow : A System for Large-Scale Machine Learning}},
year = {2016}
}
@article{Sokolic2016,
author = {Sokolic, Jure and Giryes, Raja and Sapiro, Guillermo and {R. D. Rodrigues}, Miguel},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sokolic et al. - 2016 - Lessons From the Rademacher Complexity for Deep Learning.pdf:pdf},
number = {1},
pages = {3--6},
title = {{Lessons From the Rademacher Complexity for Deep Learning}},
year = {2016}
}
@article{Canziani2016,
abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, oper-ations count, inference time and power consumption. Key findings are: (1) fully connected layers are largely inefficient for smaller batches of images; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint are an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1605.07678v4},
author = {Canziani, Alfredo and Culurciello, Eugenio and Paszke, Adam},
eprint = {arXiv:1605.07678v4},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canziani, Culurciello, Paszke - 2016 - An Analysis of Deep Neural Network Models for Practical Applications.pdf:pdf},
isbn = {2857825749},
journal = {Computing Research Repository},
month = {may},
pages = {1--7},
title = {{An Analysis of Deep Neural Network Models for Practical Applications}},
url = {http://arxiv.org/abs/1605.07678 https://arxiv.org/pdf/1605.07678.pdf},
volume = {abs/1605.0},
year = {2016}
}
@article{Louizos2017,
abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by taking a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1705.08665},
author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
eprint = {1705.08665},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Louizos, Ullrich, Welling - 2017 - Bayesian Compression for Deep Learning.pdf:pdf},
month = {may},
title = {{Bayesian Compression for Deep Learning}},
url = {http://arxiv.org/abs/1705.08665},
year = {2017}
}
@article{Hardt2017,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model fam-ily, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional ap-proaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a ran-dom labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by com-pletely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks al-ready have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03530v1},
author = {Hardt, Moritz and Vinyals, Oriol},
doi = {10.1109/TKDE.2015.2507132},
eprint = {arXiv:1611.03530v1},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardt, Vinyals - 2017 - Understanding Deep Learning Requires Re- Thinking Generalization.pdf:pdf},
isbn = {0897915240},
issn = {10414347},
journal = {International Conference on Learning Representations 2017},
month = {nov},
pages = {1--15},
pmid = {9377276},
title = {{Understanding Deep Learning Requires Re- Thinking Generalization}},
url = {http://arxiv.org/abs/1611.03530 https://arxiv.org/pdf/1611.03530.pdf},
year = {2017}
}
@article{Hoffer2017,
abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
archivePrefix = {arXiv},
arxivId = {1705.08741},
author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
eprint = {1705.08741},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffer, Hubara, Soudry - 2017 - Train longer, generalize better closing the generalization gap in large batch training of neural network.pdf:pdf},
month = {may},
title = {{Train longer, generalize better: closing the generalization gap in large batch training of neural networks}},
url = {http://arxiv.org/abs/1705.08741},
year = {2017}
}
@article{Bartlett2017,
abstract = {We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting {\$}W{\$} be the number of weights and {\$}L{\$} be the number of layers, we prove that the VC-dimension is {\$}O(W L \backslashlog(W)){\$}, and provide examples with VC-dimension {\$}\backslashOmega( W L \backslashlog(W/L) ){\$}. This improves both the previously known upper bounds and lower bounds. In terms of the number {\$}U{\$} of non-linear units, we prove a tight bound {\$}\backslashTheta(W U){\$} on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial.},
archivePrefix = {arXiv},
arxivId = {1703.02930},
author = {Bartlett, Peter L and Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
eprint = {1703.02930},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bartlett et al. - 2017 - Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks.pdf:pdf},
keywords = {ReLU activation function,VC-dimension,neural networks,pseudodimension,statistical learning theory},
title = {{Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks}},
url = {http://proceedings.mlr.press/v65/harvey17a.html; http://arxiv.org/abs/1703.02930},
year = {2017}
}
@article{Zhang2017,
abstract = {Hyperparameter tuning is one of the big costs of deep learning. State-of-the-art optimizers, such as Adagrad, RMSProp and Adam, make things easier by adaptively tuning an individual learning rate for each variable. This level of fine adaptation is understood to yield a more powerful method. However, our experiments, as well as recent theory by Wilson et al., show that hand-tuned stochastic gradient descent (SGD) achieves better results, at the same rate or faster. The hypothesis put forth is that adaptive methods converge to different minima (Wilson et al.). Here we point out another factor: none of these methods tune their momentum parameter, known to be very important for deep learning applications (Sutskever et al.). Tuning the momentum parameter becomes even more important in asynchronous-parallel systems: recent theory (Mitliagkas et al.) shows that asynchrony introduces momentum-like dynamics, and that tuning down algorithmic momentum is important for efficient parallelization. We revisit the simple momentum SGD algorithm and show that hand-tuning a single learning rate and momentum value makes it competitive with Adam. We then analyze its robustness in learning rate misspecification and objective curvature variation. Based on these insights, we design YellowFin, an automatic tuner for both momentum and learning rate in SGD. YellowFin optionally uses a novel momentum-sensing component along with a negative-feedback loop mechanism to compensate for the added dynamics of asynchrony on the fly. We empirically show YellowFin converges in fewer iterations than Adam on large ResNet and LSTM models, with a speedup up to {\$}2.8{\$}x in synchronous and {\$}2.7{\$}x in asynchronous settings.},
archivePrefix = {arXiv},
arxivId = {1706.03471},
author = {Zhang, Jian and Mitliagkas, Ioannis and R{\'{e}}, Christopher},
eprint = {1706.03471},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Mitliagkas, R{\'{e}} - 2017 - YellowFin and the Art of Momentum Tuning.pdf:pdf},
month = {jun},
title = {{YellowFin and the Art of Momentum Tuning}},
url = {http://arxiv.org/abs/1706.03471},
year = {2017}
}
@article{Sønderby2015,
abstract = {We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5{\%} compared to 2.9{\%} for a convolutional networks and 2.0{\%} for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest.},
archivePrefix = {arXiv},
arxivId = {1509.05329},
author = {S{\o}nderby, S{\o}ren Kaae and S{\o}nderby, Casper Kaae and Maal{\o}e, Lars and Winther, Ole},
doi = {10.1038/nbt.3343},
eprint = {1509.05329},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2015 - Spatial Transformer Networks.pdf:pdf},
isbn = {9781627480031},
issn = {1087-0156},
month = {jun},
pmid = {26571099},
title = {{Recurrent Spatial Transformer Networks}},
url = {http://arxiv.org/abs/1506.02025 http://arxiv.org/abs/1509.05329},
year = {2015}
}
@inproceedings{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74{\%} test error on CIFAR-10, 19.25{\%} on CIFAR-100 and 1.59{\%} on SVHN).},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and van der Maaten, Laurens and Weinberger, Kilian Q.},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Liu, Weinberger - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {1--12},
pmid = {211888},
publisher = {IEEE},
title = {{Densely Connected Convolutional Networks}},
url = {http://ieeexplore.ieee.org/document/8099726/ http://arxiv.org/abs/1608.06993},
year = {2017}
}
@article{Cun,
abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cun, Yann Le and Denker, John S and Solla, Sara a},
doi = {10.1.1.32.7223},
eprint = {arXiv:1011.1669v3},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cun, Denker, Solla - 1990 - Optimal Brain Damage.pdf:pdf},
isbn = {1558601007},
issn = {1098-6596},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {598--605},
pmid = {25246403},
title = {{Optimal Brain Damage}},
url = {https://papers.nips.cc/paper/250-optimal-brain-damage.pdf},
volume = {2},
year = {1990}
}
@article{Sabour,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.09829v1},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E and Toronto, Google Brain},
eprint = {arXiv:1710.09829v1},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sabour et al. - Unknown - Dynamic Routing Between Capsules.pdf:pdf},
title = {{Dynamic Routing Between Capsules}},
url = {https://arxiv.org/pdf/1710.09829.pdf}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
eprint = {arXiv:1011.1669v3},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
pmid = {25246403},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Zhang2016,
abstract = {A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks is met with a diminishing return and increased training difficulty; on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9{\%} to 4.25{\%}, and that on multi-crops from 3.7{\%} to 3.45{\%}.},
archivePrefix = {arXiv},
arxivId = {1611.05725},
author = {Zhang, Xingcheng and Li, Zhizhong and Loy, Chen Change and Lin, Dahua},
doi = {10.1109/CVPR.2017.415},
eprint = {1611.05725},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2016 - PolyNet A Pursuit of Structural Diversity in Very Deep Networks.pdf:pdf},
isbn = {1-5386-0457-4},
title = {{PolyNet: A Pursuit of Structural Diversity in Very Deep Networks}},
url = {https://arxiv.org/pdf/1611.05725.pdf http://arxiv.org/abs/1611.05725},
year = {2016}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
month = {sep},
pages = {1--9},
pmid = {24920543},
title = {{Going deeper with convolutions}},
url = {http://arxiv.org/abs/1409.4842},
volume = {07-12-June},
year = {2015}
}
@article{Zhang2017a,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model fam-ily, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional ap-proaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a ran-dom labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by com-pletely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks al-ready have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03530v1},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
doi = {10.1109/TKDE.2015.2507132},
eprint = {arXiv:1611.03530v1},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardt, Vinyals - 2017 - Understanding Deep Learning Requires Re- Thinking Generalization.pdf:pdf},
isbn = {0897915240},
issn = {10414347},
journal = {International Conference on Learning Representations 2017},
month = {nov},
pages = {1--15},
pmid = {9377276},
title = {{Understanding Deep Learning Requires Re- Thinking Generalization}},
url = {http://arxiv.org/abs/1611.03530 https://arxiv.org/pdf/1611.03530.pdf},
year = {2017}
}
@article{Orabona2017,
abstract = {Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.},
author = {Orabona, Francesco and Tommasi, Tatiana},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Orabona, Tommasi - 2017 - Training Deep Networks without Learning Rates Through Coin Betting.pdf:pdf},
title = {{Training Deep Networks without Learning Rates Through Coin Betting}},
url = {https://arxiv.org/pdf/1705.07795.pdf},
year = {2017}
}
@article{Gao2016,
abstract = {Great successes of deep neural networks have been witnessed in various real applications. Many algorithmic and implementation techniques have been developed, however, theoretical understanding of many aspects of deep neural networks is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity of different types of dropout, and our theoretical results disclose that for shallow neural networks (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for deep neural networks it can amazingly lead to an exponential reduction of the Rademacher complexity.},
archivePrefix = {arXiv},
arxivId = {1402.3811},
author = {Gao, Wei and Zhou, Zhi Hua},
doi = {10.1007/s11432-015-5470-z},
eprint = {1402.3811},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao, Zhou - 2016 - Dropout Rademacher complexity of deep neural networks.pdf:pdf},
issn = {1674733X},
journal = {Science China Information Sciences},
keywords = {Rademacher complexity,artificial intelligence,deep learning,dropout,machine learning},
number = {7},
title = {{Dropout Rademacher complexity of deep neural networks}},
url = {https://arxiv.org/pdf/1402.3811.pdf},
volume = {59},
year = {2016}
}
@inproceedings{Mnih2010,
abstract = {Reliably extracting information fromaerial imagery is a difficult prob- lemwith many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detec- tion system is currently on the market and no published method has been shown to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The net- work is trained on massive amounts of data using a consumer GPU.We demon- strate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels.We show that our method works reliably on two challenging urban datasets that are an order ofmagnitude larger than what was used to evaluate previous approaches. 1},
author = {Mnih, Volodymyr and Hinton, Geoffrey E},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15567-3_16},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih, Hinton - 2010 - Learning to detect roads in high-resolution aerial images.pdf:pdf},
isbn = {3642155669},
issn = {03029743},
number = {PART 6},
pages = {210--223},
title = {{Learning to detect roads in high-resolution aerial images}},
volume = {6316 LNCS},
year = {2010}
}
@article{Alvarez2017,
abstract = {In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.},
archivePrefix = {arXiv},
arxivId = {1711.02638},
author = {Alvarez, Jose M and Salzmann, Mathieu},
eprint = {1711.02638},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez, Salzmann - 2017 - Compression-aware Training of Deep Networks.pdf:pdf},
title = {{Compression-aware Training of Deep Networks}},
url = {http://papers.nips.cc/paper/6687-compression-aware-training-of-deep-networks.pdf http://arxiv.org/abs/1711.02638},
year = {2017}
}
@article{Crankshaw2016,
abstract = {Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment. In this paper, we introduce Clipper, a general-purpose low-latency prediction serving system. Interposing between end-user applications and a wide range of machine learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks and applications. Furthermore, by introducing caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark datasets and demonstrate its ability to meet the latency, accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the TensorFlow Serving system and demonstrate that we are able to achieve comparable throughput and latency while enabling model composition and online learning to improve accuracy and render more robust predictions.},
archivePrefix = {arXiv},
arxivId = {1612.03079},
author = {Crankshaw, Daniel and Wang, Xin and Zhou, Giulio and Franklin, Michael J and Gonzalez, Joseph E and Stoica, Ion},
eprint = {1612.03079},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crankshaw et al. - 2016 - Clipper A Low-Latency Online Prediction Serving System.pdf:pdf},
isbn = {978-1-931971-37-9},
title = {{Clipper: A Low-Latency Online Prediction Serving System}},
url = {https://rise.cs.berkeley.edu/wp-content/uploads/2017/02/clipper{\_}final.pdf http://arxiv.org/abs/1612.03079},
year = {2016}
}
@article{Ciresan2010,
abstract = {Good old on-line back-propagation for plain multi-layer perceptrons yields a very low 0.35{\%} error rate on the famous MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images, and graphics cards to greatly speed up learning.},
archivePrefix = {arXiv},
arxivId = {1003.0358},
author = {Ciresan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Juergen},
doi = {10.1162/NECO_a_00052},
eprint = {1003.0358},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ciresan et al. - 2010 - Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition.pdf:pdf},
isbn = {1530-888X (Electronic)$\backslash$r0899-7667 (Linking)},
issn = {1530-888X},
journal = {Neural Computation},
month = {mar},
number = {12},
pages = {3207--3220},
pmid = {20858131},
title = {{Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition}},
url = {http://arxiv.org/abs/1003.0358{\%}0Ahttp://dx.doi.org/10.1162/NECO{\_}a{\_}00052},
volume = {22},
year = {2010}
}
@inproceedings{Ng2004,
abstract = {We consider supervised learning in the pres-ence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logis-tic regression, we show that using L 1 regu-larization of the parameters, the sample com-plexity (i.e., the number of training examples required to learn " well, ") grows only loga-rithmically in the number of irrelevant fea-tures. This logarithmic rate matches the best known bounds for feature selection, and in-dicates that L 1 regularized logistic regression can be effective even if there are exponen-tially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invari-ant algorithm—including logistic regression with L 2 regularization, SVMs, and neural networks trained by backpropagation—has a worst case sample complexity that grows at least linearly in the number of irrelevant fea-tures.},
author = {Ng, Andrew Y},
booktitle = {ICML},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2004 - Feature selection, ell{\_}1 vs. ell{\_}2 regularization, and rotational invariance.pdf:pdf},
pages = {78--85},
title = {{Feature selection, $\backslash$ell{\_}1 vs. $\backslash$ell{\_}2 regularization, and rotational invariance.}},
url = {http://www.machinelearning.org/proceedings/icml2004/papers/354.pdf},
year = {2004}
}
@inproceedings{Xu2016,
abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
annote = {Theu suggest that ReLU is better than tanh and sigmoid and produce sparsity in the activations. Everything is true and is now widely accepted. However it does not benefit the project.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Xu, Lie and Choy, Chiu Sing and Li, Yi Wen},
booktitle = {2016 International Workshop on Acoustic Signal Enhancement, IWAENC 2016},
doi = {10.1109/IWAENC.2016.7602891},
eprint = {1502.03167},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Choy, Li - 2016 - Deep sparse rectifier neural networks for speech denoising.pdf:pdf},
isbn = {9781509020072},
issn = {15324435},
keywords = {Network pruning,Rectifier neurons,Sparseness,Speech denoising},
pages = {315--323},
title = {{Deep sparse rectifier neural networks for speech denoising}},
url = {https://www.utc.fr/{~}bordesan/dokuwiki/{\_}media/en/glorot10nipsworkshop.pdf},
volume = {15},
year = {2016}
}
@article{Li2017,
abstract = {Generative Adversarial Networks (GANs) have recently been proposed as a promising avenue towards learning generative models with deep neural networks. While GANs have demonstrated state-of-the-art performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we take a first step towards a rigorous study of GAN dynamics. We propose a simple model that exhibits several of the common problematic convergence behaviors (e.g., vanishing gradient, mode collapse, diverging or oscillatory behavior) and still allows us to establish the first convergence bounds for parametric GAN dynamics. We find an interesting dichotomy: a GAN with an optimal discriminator provably converges, while a first order approximation of the discriminator leads to unstable GAN dynamics and mode collapse. Our model and analysis point to a specific challenge in practical GAN training that we call discriminator collapse.},
archivePrefix = {arXiv},
arxivId = {1706.09884},
author = {Li, Jerry and Madry, Aleksander and Peebles, John and Schmidt, Ludwig},
eprint = {1706.09884},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Towards Understanding the Dynamics of Generative Adversarial Networks.pdf:pdf},
month = {jun},
title = {{Towards Understanding the Dynamics of Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1706.09884},
year = {2017}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
doi = {10.1051/0004-6361/201527329},
eprint = {1603.07285},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Arxiv},
pages = {1--28},
pmid = {26353135},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@misc{NVIDIA,
author = {NVIDIA},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/NVIDIA - Unknown - NVIDIA Tesla V100 GPU accelerator.pdf:pdf},
title = {{NVIDIA Tesla V100 GPU accelerator}},
url = {https://images.nvidia.com/content/technologies/volta/pdf/437317-Volta-V100-DS-NV-US-WEB.pdf},
urldate = {2018-03-09}
}
@article{Keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say {\$}32{\$}-{\$}512{\$} data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tak, Ping and Tang, Ping Tak Peter},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {1609.04836},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keskar et al. - 2014 - ON LARGE-BATCH TRAINING FOR DEEP LEARNING GENERALIZATION GAP AND SHARP MINIMA.pdf:pdf},
isbn = {9781405161251},
issn = {1607-551X},
month = {sep},
number = {2},
pages = {1--134},
pmid = {17460516},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
url = {http://arxiv.org/abs/1609.04836 https://arxiv.org/pdf/1609.04836.pdf},
year = {2016}
}
@article{Scardapane2017,
abstract = {In this paper, we address the challenging task of simultaneously optimizing (i) the weights of a neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are traditionally dealt with separately, we propose an efficient regularized formulation enabling their simultaneous parallel execution, using standard optimization routines. Specifically, we extend the group Lasso penalty, originally proposed in the linear regression literature, to impose group-level sparsity on the network's connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We carry out an extensive experimental evaluation, in comparison with classical weight decay and Lasso penalties, both on a toy dataset for handwritten digit recognition, and multiple realistic mid-scale classification benchmarks. Comparative results demonstrate the potential of our proposed sparse group Lasso penalty in producing extremely compact networks, with a significantly lower number of input features, with a classification accuracy which is equal or only slightly inferior to standard regularization terms.},
archivePrefix = {arXiv},
arxivId = {1607.00485},
author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
doi = {10.1016/j.neucom.2017.02.029},
eprint = {1607.00485},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scardapane et al. - 2017 - Group sparse regularization for deep neural networks.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Deep networks,Feature selection,Group sparsity,Pruning},
pages = {81--89},
title = {{Group sparse regularization for deep neural networks}},
url = {https://arxiv.org/pdf/1607.00485.pdf},
volume = {241},
year = {2017}
}
@inproceedings{JiaDeng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {{Jia Deng} and {Wei Dong} and Socher, Richard and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPRW.2009.5206848},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://www.image-net.org/papers/imagenet{\_}cvpr09.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
year = {2009}
}
@inproceedings{Zoph2017b,
abstract = {Neural networks are powerful and flexible models that work well for many diffi- cult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a re- current network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.84, which is only 0.1 percent worse and 1.2x faster than the current state-of-the-art model. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of- the-art.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V.},
booktitle = {ICLR},
eprint = {1611.01578},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph, Le - 2017 - Neural Architecture Search with Reinforcement Learning.pdf:pdf},
isbn = {9781424425051},
month = {nov},
pages = {976--981},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01578},
year = {2017}
}
@article{Ravi2017,
abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTM- based meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametriza- tion of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick con- vergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
author = {Ravi, Sachin and Larochelle, Hugo},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravi, Larochelle - 2017 - Optimization As a Model for Few-Shot Learning.pdf:pdf},
journal = {ICLR},
pages = {1--11},
title = {{Optimization As a Model for Few-Shot Learning}},
url = {https://openreview.net/pdf?id=rJY0-Kcll},
year = {2017}
}
@article{Han2015,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J},
doi = {abs/1510.00149/1510.00149},
eprint = {1510.00149},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Mao, Dally - 2015 - Deep Compression Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.pdf:pdf},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
url = {https://arxiv.org/pdf/1510.00149.pdf http://arxiv.org/abs/1510.00149},
year = {2015}
}
@misc{Pan2010,
abstract = {A survey on transfer learning. IEEE Trans. Knowl. Data Eng},
author = {Pan, Sinno Jialin and Yang, Qiang},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2009.191},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Yang - 2010 - A survey on transfer learning.pdf:pdf},
isbn = {1041-4347 VO - 22},
issn = {10414347},
keywords = {Transfer learning,data mining.,machine learning,survey},
number = {10},
pages = {1345--1359},
title = {{A survey on transfer learning}},
volume = {22},
year = {2010}
}
@inproceedings{Huang2016,
abstract = {Very deep convolutional networks with hundreds or more layers have lead to significant reductions in error on competitive benchmarks like the ImageNet or COCO tasks. Although the unmatched expressiveness of the many deep layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes and the training time can be painfully slow even on modern computers. In this paper we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and obtain deep networks. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. The resulting networks are short (in expectation) during training and deep during testing. Training Residual Networks with stochastic depth is compellingly simple to implement, yet effective. We show that this approach successfully addresses the training difficulties of deep networks and complements the recent success of Residual and Highway Networks. It reduces training time substantially and improves the test errors on almost all data sets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, we show that with stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91{\%}) on CIFAR-10.},
archivePrefix = {arXiv},
arxivId = {1603.09382},
author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_39},
eprint = {1603.09382},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2016 - Deep networks with stochastic depth.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
month = {mar},
pages = {646--661},
pmid = {4520227},
title = {{Deep networks with stochastic depth}},
url = {http://arxiv.org/abs/1603.09382},
volume = {9908 LNCS},
year = {2016}
}
@article{Wilson2017,
abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
archivePrefix = {arXiv},
arxivId = {1705.08292},
author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
eprint = {1705.08292},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson et al. - 2017 - The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf:pdf},
month = {may},
title = {{The Marginal Value of Adaptive Gradient Methods in Machine Learning}},
url = {http://arxiv.org/abs/1705.08292},
year = {2017}
}
@inproceedings{Jouppi2017,
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a ​ Tensor Processing Unit (TPU) ​ — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, {\ldots}) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95{\%} of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU. Index terms–DNN, MLP, CNN, RNN, LSTM, neural network, domain-specific architecture, accelerator},
archivePrefix = {arXiv},
arxivId = {1704.04760},
author = {Jouppi, Norman P. and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Young, Cliff and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Patil, Nishant and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Patterson, David and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Agrawal, Gaurav and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Bajwa, Raminder and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Bates, Sarah and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun and Bhatia, Suresh and Boden, Nan},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture - ISCA '17},
doi = {10.1145/3079856.3080246},
eprint = {1704.04760},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Processing Unit.pdf:pdf},
isbn = {9781450348928},
issn = {10636897},
month = {apr},
pages = {1--12},
title = {{In-Datacenter Performance Analysis of a Tensor Processing Unit}},
url = {http://arxiv.org/abs/1704.04760 http://dl.acm.org/citation.cfm?doid=3079856.3080246},
year = {2017}
}
@article{Lebedev2015,
abstract = {We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion by adding group-sparsity regularization to the standard training process. After such group-wise pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. In the comparison on AlexNet, the method achieves very competitive performance.},
archivePrefix = {arXiv},
arxivId = {1506.02515},
author = {Lebedev, Vadim and Lempitsky, Victor},
doi = {10.1109/CVPR.2016.280},
eprint = {1506.02515},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lebedev, Lempitsky - 2015 - Fast ConvNets Using Group-wise Brain Damage.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {jun},
pages = {2554--2564},
publisher = {IEEE},
title = {{Fast ConvNets Using Group-wise Brain Damage}},
url = {http://ieeexplore.ieee.org/document/7780649/ http://arxiv.org/abs/1506.02515},
year = {2015}
}
@article{Cardot2015,
abstract = {In the current context of data explosion, online techniques that do not require storing all data in memory are indispensable to routinely perform tasks like principal component analysis (PCA). Recursive algorithms that update the PCA with each new observation have been studied in various fields of research and found wide applications in industrial monitoring, computer vision, astronomy, and latent semantic indexing, among others. This work provides guidance for selecting an online PCA algorithm in practice. We present the main approaches to online PCA, namely, perturbation techniques, incremental methods, and stochastic optimization, and compare their statistical accuracy, computation time, and memory requirements using artificial and real data. Extensions to missing data and to functional data are discussed. All studied algorithms are available in the R package onlinePCA on CRAN.},
archivePrefix = {arXiv},
arxivId = {1511.03688},
author = {Cardot, Herv{\'{e}} and Degras, David},
doi = {10.1111/insr.12220},
eprint = {1511.03688},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cardot, Degras - 2015 - Online Principal Component Analysis in High Dimension Which Algorithm to Choose.pdf:pdf},
issn = {17515823},
keywords = {Covariance matrix,Eigendecomposition,Generalized hebbian algorithm,Incremental SVD,Perturbation methods,Recursive algorithms,Stochastic gradient},
title = {{Online Principal Component Analysis in High Dimension: Which Algorithm to Choose?}},
url = {https://arxiv.org/pdf/1511.03688.pdf http://arxiv.org/abs/1511.03688},
year = {2015}
}
@article{Goyal2017,
abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\~{}}90{\%} scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internet-scale data with high efficiency.},
archivePrefix = {arXiv},
arxivId = {1706.02677},
author = {Goyal, Priya and Doll{\'{a}}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
eprint = {1706.02677},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal et al. - 2017 - Accurate, Large Minibatch SGD Training ImageNet in 1 Hour.pdf:pdf},
month = {jun},
title = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
url = {http://arxiv.org/abs/1706.02677},
year = {2017}
}
@inproceedings{Luo2017,
abstract = {We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31{\$}\backslashtimes{\$} FLOPs reduction and 16.63{\$}\backslashtimes{\$} compression on VGG-16, with only 0.52{\$}\backslash{\%}{\$} top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1{\$}\backslash{\%}{\$} top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.},
archivePrefix = {arXiv},
arxivId = {1707.06342},
author = {Luo, Jian Hao and Wu, Jianxin and Lin, Weiyao},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.541},
eprint = {1707.06342},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Wu, Lin - 2017 - ThiNet A Filter Level Pruning Method for Deep Neural Network Compression.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
month = {oct},
pages = {5068--5076},
publisher = {IEEE},
title = {{ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression}},
url = {http://ieeexplore.ieee.org/document/8237803/},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{He2016a,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning},
month = {dec},
pages = {770--778},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/He{\_}Deep{\_}Residual{\_}Learning{\_}CVPR{\_}2016{\_}paper.pdf http://ieeexplore.ieee.org/document/7780459/ http://arxiv.org/abs/1512.03385},
year = {2016}
}
@inproceedings{Lecun1998,
abstract = {| Multilayer Neural Networks trained with the backpropa-gation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten char-acters, with minimal preprocessing. This paper reviews var-ious methods applied to handwritten character recognition and compares them on a standard handwritten digit recog-nition task. Convolutional Neural Networks, that are specif-ically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including eld extraction, segmenta-tion, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), al-lows such m ulti-module systems to be trained globally using Gradient-Based methods so as to minimize an overall per-formance measure. Two systems for on-line handwriting recognition are de-scribed. Experiments demonstrate the advantage of global training, and the eexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check i s also described. It uses Convolutional Neural Network char-acter recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks perday.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Y and Bottou, L and Bengio, Yoshua and Haffner, P},
booktitle = {Intelligent Signal Processing},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 2001 - Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
pages = {306--351},
pmid = {15823584},
title = {{Gradient-Based Learning Applied to Document Recognition}},
url = {http://vision.stanford.edu/cs598{\_}spring07/papers/Lecun98.pdf},
year = {2001}
}
@inproceedings{Sainath2013,
abstract = {While Deep Neural Networks (DNNs) have achieved tremendous success for large vocabulary continuous speech recognition (LVCSR) tasks, training of these networks is slow. One reason is that DNNs are trained with a large number of training parameters (i.e., 10-50 million). Because networks are trained with a large number of output targets to achieve good performance, the majority of these parameters are in the final weight layer. In this paper, we propose a low-rank matrix factorization of the final weight layer. We apply this low-rank technique to DNNs for both acoustic modeling and language modeling. We show on three different LVCSR tasks ranging between 50-400 hrs, that a low-rank factorization reduces the number of parameters of the network by 30-50{\%}. This results in roughly an equivalent reduction in training time, without a significant loss in final recognition accuracy, compared to a full-rank representation.},
author = {Sainath, Tara N. and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6638949},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sainath et al. - 2013 - Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
pages = {6655--6659},
title = {{Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets}},
url = {http://ieeexplore.ieee.org/document/6638949/},
year = {2013}
}
@article{Roche2005,
abstract = {Corporate learning programs are under pressure to show a better return on investment, even as evidence mounts that classroom training may not be enough to ensure transfer of learning to the job. To improve its bottom-line impact, the Global Learning and Leadership Development group at Agilent Technologies stepped outside the “classroom box” to focus on the learning period after formal instruction ends. It implemented a new follow-through system and other innovations to encourage on-the-job application of course learning and to facilitate coaching and feedback, particularly from participants'managers, who can make the difference in successful learning transfer. {\textcopyright} 2005 Wiley Periodicals, Inc. [ABSTRACT FROM AUTHOR]},
archivePrefix = {arXiv},
arxivId = {1701.06106},
author = {Roche, Teresa and Wick, Cal and Stewart, Marie},
doi = {10.1002/joe.20070},
eprint = {1701.06106},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roche, Wick, Stewart - 2005 - Innovation in learning Agilent technologies thinks outside the box.pdf:pdf},
isbn = {15311864},
issn = {15311864},
journal = {Journal of Organizational Excellence},
month = {oct},
number = {4},
pages = {45},
title = {{Innovation in learning: Agilent technologies thinks outside the box}},
url = {http://arxiv.org/abs/1710.10196},
volume = {24},
year = {2005}
}
@article{Alvarez2016,
abstract = {Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80$\backslash${\%} while retaining or even improving the network accuracy.},
archivePrefix = {arXiv},
arxivId = {1611.06321},
author = {Alvarez, Jose M and Salzmann, Mathieu},
eprint = {1611.06321},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez, Salzmann - 2016 - Learning the Number of Neurons in Deep Networks.pdf:pdf},
issn = {10495258},
title = {{Learning the Number of Neurons in Deep Networks}},
url = {http://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks.pdf http://arxiv.org/abs/1611.06321},
year = {2016}
}
@article{Shapiro1965,
abstract = {Accessed: 07-03-2018 02:55 UTC JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
author = {Shapiro, Samuel and Wilk, Martin},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shapiro, Wilk - 1965 - An Analysis of Variance Test for Normality.pdf:pdf},
journal = {Biometrika},
number = {3/4},
pages = {591--611},
publisher = {Oxford University Press},
title = {{An Analysis of Variance Test for Normality}},
url = {http://www.jstor.org/stable/2333709 http://www.jstor.org/discover/10.2307/2333709?uid=3737760{\&}uid=2{\&}uid=4{\&}sid=21104809861943},
volume = {52},
year = {1965}
}
@article{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Us-ing a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on ob-ject detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts de-tections for more than 9000 different object categories. And it still runs in real-time.},
author = {Redmon, Joseph},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon - 2017 - YOLO9000.pdf:pdf},
journal = {arxiv},
number = {April},
title = {{YOLO9000}},
url = {http://pjreddie.com/yolo9000/},
year = {2017}
}
@incollection{GoodfellowIanBengioYoshuaCourville2016RNN,
abstract = {The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free.},
author = {{Goodfellow, Ian, Bengio, Yoshua, Courville}, Aaron},
booktitle = {MIT Press},
chapter = {10},
isbn = {0262035618},
title = {{Sequence Modeling: Recurrent and Recursive Nets}},
url = {http://www.deeplearningbook.org/},
year = {2016}
}
@article{Klambauer2017,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
doi = {1706.02515},
eprint = {1706.02515},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:pdf},
title = {{Self-Normalizing Neural Networks}},
url = {https://arxiv.org/pdf/1706.02515.pdf http://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{Aghasi2016,
abstract = {We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program. This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model. The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm. While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner. In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model. To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length {\$}N{\$} as inputs, we show that if the network response can be described using a maximum number of {\$}s{\$} non-zero weights per node, these weights can be learned from {\$}\backslashmathcal{\{}O{\}}(s\backslashlog N){\$} samples.},
archivePrefix = {arXiv},
arxivId = {1611.05162},
author = {Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},
eprint = {1611.05162},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aghasi et al. - 2016 - Net-Trim Convex Pruning of Deep Neural Networks with Performance Guarantee.pdf:pdf},
title = {{Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee}},
url = {https://papers.nips.cc/paper/6910-net-trim-convex-pruning-of-deep-neural-networks-with-performance-guarantee.pdf http://arxiv.org/abs/1611.05162},
year = {2016}
}
@inproceedings{Philipp,
abstract = {Automatically determining the optimal size of a neural network for a given task without requiring input from the user currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of finding a good network size during a single training cycle. We introduce nonpara- metric neural networks, a non-probabilistic framework for conducting optimiza- tion over all possible network sizes. We prove its soundness when network growth is limited via an ?p penalty. We train networks under this framework by continu- ously adding new units and employing a novel optimization algorithm, which we term “Adaptive Radial-Angular Gradient Descent” or AdaRad. We obtain promis- ing results.},
archivePrefix = {arXiv},
arxivId = {1712.05440},
author = {Philipp, George and Carbonell, Jaime G},
booktitle = {Proc. International Conference on Learning Representations},
eprint = {1712.05440},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Philipp, Carbonell - 2017 - Nonparametric Neural Network.pdf:pdf},
number = {2016},
pages = {1--27},
title = {{Nonparametric Neural Network}},
url = {https://www.cs.cmu.edu/{~}jgc/publication/Nonparametric Neural Networks.pdf https://openreview.net/pdf?id=BJK3Xasel},
year = {2017}
}
@misc{Tibshirani1996,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society B},
doi = {10.2307/2346178},
eprint = {11/73273},
isbn = {0849320240},
issn = {00359246},
number = {1},
pages = {267--288},
pmid = {16272381},
primaryClass = {1369–7412},
title = {{Regression Selection and Shrinkage via the Lasso}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7574},
volume = {58},
year = {1996}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the $\backslash$textit{\{}Information Plane{\}}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\{}$\backslash$emph compression{\}} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shwartz-Ziv, Tishby - 2017 - Opening the Black Box of Deep Neural Networks via Information.pdf:pdf},
journal = {arXiv},
keywords = {Deep Learning,Deep Neural Networks,Information Bottleneck,Representation Learning},
month = {mar},
pages = {1--19},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
url = {http://arxiv.org/abs/1703.00810 https://arxiv.org/pdf/1703.00810.pdf{\%}0Ahttp://arxiv.org/abs/1703.00810},
year = {2017}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
keywords = {()},
month = {sep},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {https://arxiv.org/pdf/1409.1556.pdf http://arxiv.org/abs/1409.1556},
year = {2015}
}
@inproceedings{Hinton2011,
abstract = {The artificial neural networks that are used to recognise shapes typcially use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered representations of the pose of the feature, like SIFT, that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instatiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adpating the features to the domain},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Hinton, Geoffrey E and Krizhevsky, Alex and Wang, Sida D},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-21735-7_6},
eprint = {9605103},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton, Krizhevsky, Wang - 2011 - Transforming auto-encoders.pdf:pdf},
isbn = {9783642217340},
issn = {03029743},
keywords = {Invariance,auto-encoder,shape representation},
number = {PART 1},
pages = {44--51},
pmid = {1000183096},
primaryClass = {cs},
title = {{Transforming auto-encoders}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/transauto6.pdf},
volume = {6791 LNCS},
year = {2011}
}
@article{Kingma2015a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {1412.6980v9},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980v9},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Myung - 2017 - Read my lips, login to the virtual world.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
journal = {International Conference on Learning Representations 2015},
pages = {1--15},
pmid = {172668},
title = {{Adam: a Method for Stochastic Optimization}},
url = {https://arxiv.org/pdf/1412.6980.pdf https://arxiv.org/abs/1412.6980},
year = {2015}
}
@inproceedings{Smithson2016,
abstract = {Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 1020 solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.},
archivePrefix = {arXiv},
arxivId = {1611.02120},
author = {Smithson, Sean C. and Yang, Guang and Gross, Warren J. and Meyer, Brett H.},
booktitle = {Proceedings of the 35th International Conference on Computer-Aided Design - ICCAD '16},
doi = {10.1145/2966986.2967058},
eprint = {1611.02120},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Smithson et al. - 2016 - Neural networks designing neural networks.pdf:pdf},
isbn = {9781450344661},
issn = {10923152},
month = {nov},
pages = {1--8},
title = {{Neural networks designing neural networks}},
url = {http://arxiv.org/abs/1611.02120 http://arxiv.org/abs/1611.02120{\%}0Ahttp://arxiv.org/abs/1611.02120{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2966986.2967058},
year = {2016}
}
@article{Hosang2016,
abstract = {Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL and ImageNet, and impact on DPM and R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detector performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.},
archivePrefix = {arXiv},
arxivId = {1502.05082},
author = {Hosang, Jan and Benenson, Rodrigo and Dollar, Piotr and Schiele, Bernt},
doi = {10.1109/TPAMI.2015.2465908},
eprint = {1502.05082},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosang et al. - 2016 - What Makes for Effective Detection Proposals.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer Vision,Detection proposals,object detection},
number = {4},
pages = {814--830},
pmid = {26959679},
title = {{What Makes for Effective Detection Proposals?}},
volume = {38},
year = {2016}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
archivePrefix = {arXiv},
arxivId = {0710.5696v2},
author = {Dempster, A.P. and Laird, N.M. and Rubin, Donald B},
doi = {http://dx.doi.org/10.2307/2984875},
eprint = {0710.5696v2},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
isbn = {0000000779},
issn = {00359246},
journal = {Journal of the Royal Statistical Society Series B Methodological},
number = {1},
pages = {1--38},
pmid = {9501024},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://links.jstor.org/sici?sici=0035-9246{\%}281977{\%}2939{\%}3A1{\%}3C1{\%}3AMLFIDV{\%}3E2.0.CO{\%}3B2-Z http://www.jstor.org/stable/10.2307/2984875},
volume = {39},
year = {1977}
}
@article{Mishkin2015,
abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.},
archivePrefix = {arXiv},
arxivId = {1511.06422},
author = {Mishkin, Dmytro and Matas, Jiri},
doi = {10.1016/0898-1221(96)87329-9},
eprint = {1511.06422},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishkin, Matas - 2015 - All you need is a good init.pdf:pdf},
isbn = {0262560992},
issn = {08981221},
journal = {International Conference on Learning Representations (ICRL)},
month = {nov},
pmid = {21595383},
title = {{All you need is a good init}},
url = {https://arxiv.org/abs/1511.06422 http://arxiv.org/abs/1511.06422},
year = {2015}
}
@article{Willia1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algoWillia, R. J. (1992). Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Machine Learning, 8(3), 229–256. https://doi.org/10.1023/A:1022672621406rithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Willia, Ronald J.},
doi = {10.1023/A:1022672621406},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - Unknown - full-text.pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
number = {3},
pages = {229--256},
pmid = {903},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@article{Kwak2002,
abstract = {Feature selection plays an important role in classifying systems such as neural networks (NNs). We use a set of attributes which are relevant, irrelevant or redundant and from the viewpoint of managing a dataset which can be huge, reducing the number of attributes by selecting only the relevant ones is desirable. In doing so, higher performances with lower computational effort is expected. In this paper, we propose two feature selection algorithms. The limitation of mutual information feature selector (MIFS) is analyzed and a method to overcome this limitation is studied. One of the proposed algorithms makes more considered use of mutual information between input attributes and output classes than the MIFS. What is demonstrated is that the proposed method can provide the performance of the ideal greedy selection algorithm when information is distributed uniformly. The computational load for this algorithm is nearly the same as that of MIFS. In addition, another feature selection algorithm using the Taguchi method is proposed. This is advanced as a solution to the question as to how to identify good features with as few experiments as possible. The proposed algorithms are applied to several classification problems and compared with MIFS. These two algorithms can be combined to complement each other's limitations. The combined algorithm performed well in several experiments and should prove to be a useful method in selecting features for classification problems.},
author = {Kwak, Nojun and Choi, Chong-Ho},
doi = {10.1109/72.977291},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwak, Choi - 2002 - Input feature selection for classification problems.pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
number = {1},
pages = {143--159},
pmid = {18244416},
title = {{Input feature selection for classification problems.}},
volume = {13},
year = {2002}
}
@article{Alvarez2016a,
abstract = {Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80$\backslash${\%} while retaining or even improving the network accuracy.},
annote = {I already reviewed that paper in the thesis: Unless I did not understand the maths, they are basically doing group sparisity with proximal gradient updates (which sole role is to improve convergence speed). Moreover they prune at the end of training. "When learning terminates, the parameters of some of the neurons will have gone to zero. We can thus remove these neurons entirely[...]" The evaluate on Imagenet-12 and VGG. Their compression factor is very low (30{\%}) and even if we trained only 26/100 epochs our accuracy is already higher than what they report)  If it was not for this reviewer I would completely discard this paper },
archivePrefix = {arXiv},
arxivId = {1611.06321},
author = {Alvarez, Jose M and Salzmann, Mathieu},
eprint = {1611.06321},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez, Salzmann - Unknown - Learning the Number of Neurons in Deep Networks.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems},
title = {{Learning the Number of Neurons in Deep Networks}},
url = {https://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks.pdf http://arxiv.org/abs/1611.06321},
year = {2016}
}
@inproceedings{Dahl2013,
abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overﬁtting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectiﬁed linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modiﬁed deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2{\%} relative improvement over a DNN trained with sigmoid units, and a 14.4{\%} relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Dahl, George E. and Sainath, Tara N. and Hinton, Geoffrey E.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2013.6639346},
eprint = {arXiv:1301.3605v3},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dahl, Sainath, Hinton - 2013 - Improving deep neural networks for LVCSR using rectified linear units and dropout.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
keywords = {Bayesian optimization,LVCSR,acoustic modeling,broadcast news,deep learning,dropout,neural networks,rectified linear units},
pages = {8609--8613},
title = {{Improving deep neural networks for LVCSR using rectified linear units and dropout}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/momentum.pdf},
year = {2013}
}
@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp($\mu$) performance criteria, for arbitrary finite input environment measures $\mu$, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. {\textcopyright} 1991.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
eprint = {arXiv:1011.1669v3},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik - 1991 - Approximation capabilities of multilayer feedforward networks.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp($\mu$) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
number = {2},
pages = {251--257},
pmid = {25246403},
title = {{Approximation capabilities of multilayer feedforward networks}},
url = {https://web.njit.edu/{~}usman/courses/cs675{\_}fall17/hornik-nn-1991.pdf},
volume = {4},
year = {1991}
}
@article{Raghu2017,
abstract = {With the continuing empirical successes of deep networks, it becomes increasingly important to develop better methods for understanding training of models and the representations learned within. In this paper we propose Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
archivePrefix = {arXiv},
arxivId = {1706.05806},
author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
eprint = {1706.05806},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raghu et al. - 2017 - SVCCA Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement.pdf:pdf},
month = {jun},
title = {{SVCCA: Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement}},
url = {http://arxiv.org/abs/1706.05806},
year = {2017}
}
@article{Hassibi1994,
abstract = {We extend Optimal Brain Surgeon (OBS) -a second-order method for pruning networks -to allow for general error mea-sures, and explore a reduced computational and storage implemen-tation via a dominant eigenspace decomposition. Simulations on nonlinear, noisy pattern classification problems reveal that OBS does lead to improved generalization, and performs favorably in comparison with Optimal Brain Damage (OBD). We find that the required retraining steps in OBD may lead to inferior generaliza-tion, a result that can be interpreted as due to injecting noise back into the system. A common technique is to stop training of a large network at the minimum validation error. We found that the test error could be reduced even further by means of OBS (but not OBD) pruning. Our results justify the t {\~{}} 0 approximation used in OBS and indicate why retraining in a highly pruned network may lead to inferior performance.},
author = {Hassibi, Babak and Stork, David G and Wolff, Gregory and Watanabe, Takahiro},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassibi et al. - 1994 - Optimal Brain Surgeon extensions and performance comparisons.pdf:pdf},
isbn = {1558603220},
journal = {Advances in Neural Information Processing Systems 6. Proceedings of the 1993 Conference},
pages = {263--270},
title = {{Optimal Brain Surgeon: extensions and performance comparisons}},
url = {https://papers.nips.cc/paper/749-optimal-brain-surgeon-extensions-and-performance-comparisons.pdf},
year = {1994}
}
@article{Pedregosa2016,
abstract = {Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.},
archivePrefix = {arXiv},
arxivId = {1602.02355},
author = {Pedregosa, Fabian},
eprint = {1602.02355},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedregosa - 2016 - Hyperparameter optimization with approximate gradient.pdf:pdf},
isbn = {9781510829008},
title = {{Hyperparameter optimization with approximate gradient}},
url = {https://arxiv.org/pdf/1602.02355.pdf http://arxiv.org/abs/1602.02355},
year = {2016}
}
@article{Yuan2006,
abstract = {Summary. We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
author = {Yuan, Ming and Lin, Yi},
doi = {10.1111/j.1467-9868.2005.00532.x},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan, Lin - 2006 - Model selection and estimation in regression with grouped variables.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Analysis of variance,Lasso,Least angle regression,Non-negative garrotte,Piecewise linear solution path},
number = {1},
pages = {49--67},
pmid = {11161800},
title = {{Model selection and estimation in regression with grouped variables}},
url = {http://pages.stat.wisc.edu/{~}myuan/papers/glasso.final.pdf},
volume = {68},
year = {2006}
}
@article{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {{\ldots} Science Department, University of Toronto, Tech. {\ldots}},
pages = {1--60},
pmid = {25246403},
title = {{Learning Multiple Layers of Features from Tiny Images}},
url = {https://www.cs.toronto.edu/{~}kriz/learning-features-2009-TR.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Multiple+Layers+of+Features+from+Tiny+Images{\#}0},
year = {2009}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1998 - Gradient-based learning applied to document recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/ielx4/5/15641/00726791.pdf?tp={\&}arnumber=726791{\&}isnumber=15641},
volume = {86},
year = {1998}
}
@article{Mitchell1996,
abstract = {Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics-particularly in machine learning, scientific modeling, and artificial life-and reviews a broad span of research, including the work of Mitchell and her colleagues. The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics.},
author = {Mitchell, Melanie},
doi = {10.1016/S0898-1221(96)90227-8},
isbn = {0262631857},
issn = {08981221},
journal = {Computers {\&} Mathematics with Applications},
number = {6},
pages = {133},
pmid = {21368999},
title = {{An introduction to genetic algorithms}},
url = {https://books.google.ca/books/about/An{\_}Introduction{\_}to{\_}Genetic{\_}Algorithms.html?id=0eznlz0TF-IC{\&}pgis=1{\%}5Cnhttp://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0262631857{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/S0898122196902278},
volume = {32},
year = {1996}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}
@article{Wen2016,
abstract = {High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25{\%} to 92.60{\%}, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around {\~{}}1{\%}. Open source code is in https://github.com/wenwei202/caffe/tree/scnn},
annote = {Again an application of Group sparsity. The novelty of this paper is that they evaluate many different parameter groupings. Most (including us) of the other paper do only channel prunning. Good eval, and related work },
archivePrefix = {arXiv},
arxivId = {1608.03665},
author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
eprint = {1608.03665},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wen et al. - Unknown - Learning Structured Sparsity in Deep Neural Networks.pdf:pdf},
journal = {Neural Information Processing Systems},
keywords = {Asynchronous interconnect,Chip multiprocessor,Energy efficiency,Globally asynchronous locally synchronous (GALS),Hardware accelerators,Interconnection architectures,Network-on-chip,Neuromorphic hardware,Real-time simulation,Spiking neural networks (SNNs),energy eff,graph analytics,neurocomputers,parallel processors,real-time distributed},
title = {{Learning Structured Sparsity in Deep Neural Networks}},
url = {https://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf http://papers.nips.cc/paper/6503-learning-structured-sparsity-in-deep-neural-networks{\%}0Ahttp://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural},
year = {2016}
}
@incollection{Bartlett2001,
abstract = {We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02293v1},
author = {Bartlett, Peter L and Mendelson, Shahar},
booktitle = {Journal of Machine Learning Research},
doi = {10.1007/3-540-44581-1_15},
eprint = {arXiv:1507.02293v1},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bartlett, Mendelson - 2001 - Rademacher and Gaussian Complexities Risk Bounds and Structural Results.pdf:pdf},
isbn = {9783540423430},
issn = {15324435},
keywords = {Data-Dependent Complexity,Error Bounds,Maxi-mum Discrepancy,Rademacher Averages},
pages = {224--240},
pmid = {19477997},
title = {{Rademacher and Gaussian Complexities: Risk Bounds and Structural Results}},
url = {http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf http://link.springer.com/10.1007/3-540-44581-1{\_}15},
volume = {3},
year = {2001}
}
@inproceedings{Liu2017,
abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.},
archivePrefix = {arXiv},
arxivId = {1708.06519},
author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
booktitle = {ICCV 2017},
eprint = {1708.06519},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2017 - Learning Efficient Convolutional Networks through Network Slimming.pdf:pdf},
month = {aug},
title = {{Learning Efficient Convolutional Networks through Network Slimming}},
url = {http://arxiv.org/abs/1708.06519},
year = {2017}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
annote = {From Duplicate 1 (Neural Architecture Search with Reinforcement Learning - Zoph, Barret; Le, Quoc V)

From Duplicate 2 (Neural Architecture Search with Reinforcement Learning - Zoph, Barret; Le, Quoc V)

Motivation: Hard and long to design NN architectures

Insight: It is possible to represent networks as a flat sequence

Solution: Use reinforcement learning to predict the sequences

Takeaway: It is interesting to work on finding optimal size

Why this paper should not have been published:

I think there is a major flaw. They use recurent neural network which are made for processing input. But they actually don't have input. I could not find in the two papers related to this technique what is the input of their network. Is it noise, is it zero ? In any case I think it is just equalivatent to learn a compressed (maybe not even compressed) vector instead of a network. I would definitely compare against genetic or other algorithms. especially since they trained 12800 architectures. (this is insane !)},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V.},
eprint = {1611.01578},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph, Le - 2017 - Neural Architecture Search with Reinforcement Learning.pdf:pdf},
isbn = {9781424425051},
issn = {1938-7228},
month = {nov},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {https://arxiv.org/pdf/1611.01578.pdf http://arxiv.org/abs/1611.01578},
year = {2016}
}
@incollection{Vysniauskas1993,
abstract = {This paper presents a methodology to estimate the optimal number of$\backslash$nlearning samples and the number of hidden units needed to obtain$\backslash$na desired accuracy of a function approximation by a feedforward network.$\backslash$nThe representation error and the generalization error, components$\backslash$nof the total approximation error are analyzed and the approximation$\backslash$naccuracy of a feedforward network is investigated as a function of$\backslash$nthe number of hidden units and the number of learning samples. Based$\backslash$non the asymptotical behaviour of the approximation error, an asymptotical$\backslash$nmodel of the error function (AMEF) is introduced of which the parameters$\backslash$ncan be determined experimentally. In combination with knowledge about$\backslash$nthe computational complexity of the learning rule an optimal learning$\backslash$nset size and number of hidden units can be found resulting in a minimum$\backslash$ncomputation time for a given desired precision of the approximation.},
author = {Vy{\v{s}}niauskas, V and Groen, Frans C A and Kr{\"{o}}se, B J A},
booktitle = {Artificial neural networks},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vy{\v{s}}niauskas, Groen, Kr{\"{o}}se - 1993 - A method for finding the optimal number of learning samples and hidden units for function approxima.pdf:pdf},
keywords = {Feedforward networks,function approximation,hidden units},
pages = {550--553},
title = {{A method for finding the optimal number of learning samples and hidden units for function approximation with a feed forward network}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-1-4471-2063-6{\_}151.pdf},
year = {1993}
}
@article{Larochelle2009,
abstract = {Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initial-ization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltz-mann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages inter-nal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'{e}}r{\^{o}}me and Lamblin, Pascal},
doi = {Doi 10.1109/Tsmcc.2012.2220963},
eprint = {1503.02531},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Larochelle et al. - 2009 - Exploring Strategies for Training Deep Neural Networks.pdf:pdf},
isbn = {3531207857},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {artificial neural networks,au-toassociators,deep belief networks,restricted Boltzmann machines,unsupervised learning},
pages = {1--40},
pmid = {18249735},
title = {{Exploring Strategies for Training Deep Neural Networks}},
url = {http://www.jmlr.org/papers/volume10/larochelle09a/larochelle09a.pdf},
volume = {1},
year = {2009}
}
@article{Han2015a,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
archivePrefix = {arXiv},
arxivId = {1506.02626},
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J},
doi = {10.1016/S0140-6736(95)92525-2},
eprint = {1506.02626},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2015 - Learning both Weights and Connections for Efficient Neural Networks.pdf:pdf},
isbn = {0470021438},
issn = {01406736},
pmid = {7491032},
title = {{Learning both Weights and Connections for Efficient Neural Networks}},
url = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf http://arxiv.org/abs/1506.02626},
year = {2015}
}
@inproceedings{Bergstra2011a,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap-proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos-sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu-ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex-pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli-able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Bergstra, James S and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
booktitle = {Advances in Neural Information Processing Systems (NIPS)},
doi = {2012arXiv1206.2944S},
eprint = {1206.2944},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra et al. - 2011 - Algorithms for hyper-parameter optimization.pdf:pdf},
isbn = {9781618395993},
issn = {10495258},
pages = {2546--2554},
pmid = {9377276},
title = {{Algorithms for hyper-parameter optimization}},
url = {https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
year = {2011}
}
@article{Sutton2012,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
doi = {10.1109/MED.2013.6608833},
eprint = {1603.02199},
isbn = {0262193981},
issn = {18726240},
journal = {Learning},
number = {9},
pages = {322},
pmid = {18255791},
title = {{Reinforcement learning}},
url = {https://books.google.com/books?id=CAFR6IBF4xYC{\&}pgis=1{\%}5Cnhttp://incompleteideas.net/sutton/book/the-book.html{\%}5Cnhttps://www.dropbox.com/s/f4tnuhipchpkgoj/book2012.pdf},
volume = {3},
year = {2012}
}
@inproceedings{Alvarez2017a,
abstract = {In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computa-tionally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a stan-dard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.},
annote = {I also reviewed it already. This is the same author as the one above. My take on this: This is an extension of the previous paper. They add an extra regularization term (the more the better it seems...) so we have now Group Sparsity and Nuclear Norm. On top of the dead neuron removal procedure at the end of training they have now an extra post-processing step where they remove low singular values.  Again this is post-training. However they tried doing a single garbage collection at epoch 15 on one dataset (section 5, last paragraph) Good point for us they evaluate also on Resnet50 and as we can see if we beat them before the deadline },
archivePrefix = {arXiv},
arxivId = {1711.02638},
author = {Alvarez, Jose M and Salzmann, Mathieu},
booktitle = {Neural Information Processing Systems},
eprint = {1711.02638},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez, Salzmann - Unknown - Compression-aware Training of Deep Networks.pdf:pdf},
pages = {1--10},
title = {{Compression-aware Training of Deep Networks}},
url = {http://papers.nips.cc/paper/6687-compression-aware-training-of-deep-networks.pdf https://papers.nips.cc/paper/6687-compression-aware-training-of-deep-networks.pdf http://arxiv.org/abs/1711.02638},
year = {2017}
}
@inproceedings{Han2016,
abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10{\^{}}4 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.},
archivePrefix = {arXiv},
arxivId = {1602.01528},
author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
booktitle = {Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016},
doi = {10.1109/ISCA.2016.30},
eprint = {1602.01528},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2016 - EIE Efficient Inference Engine on Compressed Deep Neural Network.pdf:pdf},
isbn = {9781467389471},
issn = {0163-5964},
keywords = {ASIC,Algorithm-Hardware co-Design,Deep Learning,Hardware Acceleration,Model Compression},
pages = {243--254},
pmid = {313940},
title = {{EIE: Efficient Inference Engine on Compressed Deep Neural Network}},
url = {https://arxiv.org/pdf/1602.01528.pdf http://arxiv.org/abs/1602.01528},
year = {2016}
}
@article{Real,
abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6{\%} (95.6{\%} for ensemble) and 77.0{\%}, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
archivePrefix = {arXiv},
arxivId = {1703.01041},
author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alexey},
eprint = {1703.01041},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Real et al. - 2017 - Large-Scale Evolution of Image Classifiers.pdf:pdf},
issn = {1938-7228},
title = {{Large-Scale Evolution of Image Classifiers}},
url = {https://arxiv.org/pdf/1703.01041.pdf http://arxiv.org/abs/1703.01041},
year = {2017}
}
@article{Vapnik1971,
abstract = {Abstract not available. {\textcopyright} Society for Industrial and Applied Mathematics},
author = {Vapnik, V and Chervonenkis, Alexey Ya.},
doi = {10.1137/1116025},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vapnik, Chervonenkis - 1971 - On the uniform convergence of relative frequencie of events to their probabilities.pdf:pdf},
issn = {0040-585X},
journal = {Theory Probab. Appl.},
month = {jan},
number = {2},
pages = {264--280},
publisher = {Society for Industrial and Applied Mathematics},
title = {{On the uniform convergence of relative frequencie of events to their probabilities}},
url = {http://epubs.siam.org/doi/10.1137/1116025},
volume = {16},
year = {1971}
}
@inproceedings{Lee2017,
abstract = {Login systems in smart devices demand multi-factor authentication for high security and at the same time, it requires simple user experience. We propose a novel application of lip-reading satisfying these requirements. We present the adequacy of lip-reading as a biometric factor by experiment. In addition, automatic lip-reader can be implemented by LSTM (Long Short Term Memory) neural network architecture with good accuracy that can translate visual utterance to password as a knowledge factor. Furthermore, our proposed method, iterative method, can improve accuracy as much as login system required. Our work achieved 93.8{\%} by single iteration from the first result (69.1{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Lee, Daehyun and Myung, Kyungsik},
booktitle = {2017 IEEE International Conference on Consumer Electronics, ICCE 2017},
doi = {10.1109/ICCE.2017.7889386},
eprint = {1412.6980},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Myung - 2017 - Read my lips, login to the virtual world.pdf:pdf},
isbn = {9781509055449},
issn = {09252312},
month = {dec},
pages = {434--435},
pmid = {172668},
title = {{Read my lips, login to the virtual world}},
url = {http://arxiv.org/abs/1412.6980},
year = {2017}
}
@inproceedings{Wen2017,
abstract = {Very large-scale Deep Neural Networks (DNNs) have achieved remarkable successes in a large variety of computer vision tasks. However, the high computation intensity of DNNs makes it challenging to deploy these models on resource-limited systems. Some studies used low-rank approaches that approximate the filters by low-rank basis to accelerate the testing. Those works directly decomposed the pre-trained DNNs by Low-Rank Approximations (LRA). How to train DNNs toward lower-rank space for more efficient DNNs, however, remains as an open area. To solve the issue, in this work, we propose Force Regularization, which uses attractive forces to enforce filters so as to coordinate more weight information into lower-rank space. We mathematically and empirically verify that after applying our technique, standard LRA methods can reconstruct filters using much lower basis and thus result in faster DNNs. The effectiveness of our approach is comprehensively evaluated in ResNets, AlexNet, and GoogLeNet. In AlexNet, for example, Force Regularization gains 2x speedup on modern GPU without accuracy loss and 4.05x speedup on CPU by paying small accuracy degradation. Moreover, Force Regularization better initializes the low-rank DNNs such that the fine-tuning can converge faster toward higher accuracy. The obtained lower-rank DNNs can be further sparsified, proving that Force Regularization can be integrated with state-of-the-art sparsity-based acceleration methods. Source code is available in https://github.com/wenwei202/caffe},
annote = {They use a very different (Physics-based) regularization. I would have a hard time finding a mathematical relationship with ShrinkNets.  I'm not familiar with LRA methods but my guess is that is is also a post-training method. On imageNet they show real results for AlexNet },
archivePrefix = {arXiv},
arxivId = {1703.09746},
author = {Wen, Wei and Xu, Cong and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.78},
eprint = {1703.09746},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wen et al. - Unknown - Coordinating Filters for Faster Deep Neural Networks.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
pages = {658--666},
title = {{Coordinating Filters for Faster Deep Neural Networks}},
url = {https://arxiv.org/pdf/1703.09746.pdf},
volume = {2017-Octob},
year = {2017}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nair, Hinton - 2010 - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/reluICML.pdf},
year = {2010}
}
@article{Raghu2017a,
abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/},
archivePrefix = {arXiv},
arxivId = {1706.05806},
author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
doi = {1706.05806},
eprint = {1706.05806},
file = {:home/guillaume/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raghu et al. - 2017 - SVCCA Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability.pdf:pdf},
journal = {Principled Approaches to Deep Learning, ICML17 Workshop},
title = {{SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability}},
url = {https://www.padl.ws/papers/Paper 22.pdf http://arxiv.org/abs/1706.05806},
year = {2017}
}
