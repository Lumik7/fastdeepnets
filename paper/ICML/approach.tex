%!TEX root=paper.tex

\section{The ShrinkNets Approach}

In this section we describe the \shrink approach, which allows us to learn
the network size as part of the training process. The key idea is the addition
of \swls after each layer in the network;  these layers
allow us to selectively deactivate neurons during training.  We start by introducing the
basic method, and then explain how to adapt the training procedure to support this new
layer. 

\subsection{Overview}

At a high-level, our approach consists of two interconnected stages.
The first stage identifies neurons that do not improve the
prediction accuracy of the network and deactivates them. 
The second stage then {\it removes} neurons from the network (explicitly
shrinking weight matrices and updating optimizer state) thus leading to 
smaller networks and also faster inference times. 

\noindent\textbf{Deactivating Neurons On-The-Fly: }During the first stage,
\shrink applies an on/off switch to every neuron of an initially over-sized
network. We model the on/off switches by multiplying each input (or output) of
each layer by a parameter $\bm{\beta}\in\{0,1\}$. A value of $0$
will deactivate the neuron, while $1$ will let the signal go through. These
switches are part of a new layer, called the \swl; this layer applies 
to fully connected as well as convolutional layers.

Our objective is to minimize the number of ``on'' switches to reduce the model size as much
as possible while preserving prediction accuracy. This can be achieved by jointly
minimizing the training loss of the network and applying an L0 norm to the 
$\bm{\beta}$ parameters of the \swl. 
Since minimizing the L0 norm is an NP-Hard problem, we instead relax the constraint
to an L1 norm by constraining $\bm{\beta}$ to be a real number
instead of a binary value.
% Because finding an optimal binary
% assignment is an NP-Hard problem, we relax the problem by allowing
%  $\bm{\beta}$ to be a real number
% instead of a binary value and constrain it using the L1 instead of L0 norm.

\noindent\textbf{Neuron Removal: } During this stage, the neurons that are
deactivated by the switch layers are actually removed from the network,
effectively shrinking the network size. This step improves inference times, as
we demonstrate in our evaluation. We choose to remove neurons at training time
because we have observed that this allows the remaining active neurons to adapt
to the new network architecture and we can avoid a post-training step to prune
deactivated neurons.
% Existing techniques focus on neuron removal
% after training, and require an extra fine-tuning process to compensate for the
% removal.
% \srm{cite something?} \gl{I think we are fine or we could actually
% cite any of these paper}

In the remainder of this section we describe in detail the switch layer as well
as and the training process for \shrink, and 
then describe the removal process in Section~\ref{neuron_killing}.

\subsection{The Switch Layer}

Let $L$ be a layer in a neural network that takes an input tensor $\bm{x}$ and
produces an output tensor $y$ of shape $\left(c \times d_1 \times \dots \times
d_n\right)$ where $c$ is the number of neurons in that layer.  For instance, for
fully connected layers, $n$=0 and the output is single dimensional vector of size $c$
(ignoring batch size for now) while for a 2-D convolutional layer, $n$=2 and $c$
is the number of output channels or feature maps.

We want to tune the size of $L$ by applying a \swl, $S$, containing $c$
switches.  The \swl is parametrized by a vector $\bm{\beta} \in \mathbb{R}^c$ such that the result
of applying $S$ to $L(\bm{x})$ is a also a tensor size $\left(c \times d_1
\times \dots \times d_n\right)$ such that: 
%Suppose we wish to tune the size of $L$ by applying a switch layer.
%A switch layer $S$ applied to the output of $L$ can be parametrized by a 
%vector 
\begin{equation} 
S_{\beta}(L(\bm{x}))_{i,...} = \bm{\beta}_iL(\bm{x})_{i, ...} \forall i \in [1\ldots c]
\end{equation}
Once passed through the switch layer, each output channel $i$
produced by $L$ is scaled by the corresponding $\bm{\beta}_i$. Note that when
$\bm{\beta}_i = 0$, the $i^{\text{th}}$ channel is multiplied by zero and will not
contribute to any computation after the switch layer. If this happens, we say
the switch layer has {\it deactivated} the neuron corresponding to channel $i$ of layer $L$.

We place \swl after each layer whose size we wish to tune; these are
typically fully connected and convolutional layers. We discuss next how to train
\shrink.

\subsection{Training \shrink} 

%To tune the size of a network, we place Switch Layers after each layer whose size
%we wish to tune; these are typically the fully connected and convolutional layers
%in a network.
%Since the switch layers are adept at deactivating neurons, we start with an
%oversized network (i.e. network with more capacity than required) and then use
%switch layers to deactivate or kill off neurons that are unnecessary.

%Formally, 
For training, we need to account for the effect of the \swls on the loss
function. The effect of \swls can be expressed in terms of a sparsity constraint
that pushes values in the $\bm{\beta}$ vector to 0. In this way, given a neural network
parameterized by weights $\bm{\theta}$ and switch layer parameters $\bm{\theta}$, we
optimize \shrink loss as follows 
\begin{equation}
\small
  L_{SN}(\bm{x},\bm{y};\bm{\theta}, \bm{\beta}) = L(\bm{x}, \bm{y}; \bm{\beta}) +
  \lambda\norm{\beta}_1 + \lambda_2\norm{\bm{\theta}}_p^p
\end{equation}
This expression augments the regular training loss with a regularization
term for the switch parameters and another on the network weights.

We next analyze why such loss function works to train \shrink and its
connection to group sparsity:

%To train networks we need start with a substantially oversized network, then we
%insert \textit{Switch Layers}  (usually after every linear or convolutional
%layer except the last one) and we sample their weight from the
%$\text{Uniform}(0, 1)$ distribution. 
% we could train the network directly using our standard loss function, and we could achieve performance equivalent to a normal neural
% network. However, our goal is to find the smallest network with reasonable
% performance. We achieve that by introducing sparsity in the parameters of the
% \textit{Switch Layers}, thus forcing the deactivation of neurons
%. Indeed, having a negative component in the $\theta$
%parameter of the filter layer permamently disable its associated feature
%\gl{Maybe redundant ? we talked about that in the previous paragraph} . 
% To obtain this sparsity, we simply redefine the loss function:
% \begin{equation}
%   L'(\bm{x},\bm{y};\bm{W}, \bm{\theta}) = L(\bm{x}, \bm{y}; \bm{W}) +
%   \lambda\norm{\theta}_1 + \lambda_2\norm{\bm{W}}_p
% \end{equation}

% The additional term $\lambda|\max(0, \theta)|$ introduces sparsity (see Lasso
% loss~\cite{Tibshirani1996}). 
% The $\lambda$ parameter, that can take any
% positive value, adjusts how aggressively the network deactivates neurons, with
% larger values indicating more aggressive deactivation.
%  The second component of the loss increases the gradient with respect to $\theta$, thus pushing its value towards zero. Neurons with little impact
% on the original loss (gradient lower than $\lambda$), will not be able to
% compete against this attraction towards zero. Because the entries in $\theta$
% with a value of $0$ or less correspond to dead neurons, $\lambda$ effectively
% controls the number of neurons/channels in the entire network. Without the last
% term our problem sounds very similar to the Group Sparsity regularization which
% is well known in the area of linear and logistic regressions. In the next
% section we will try to undercover the relationship between these two problems,
% explain why we need this additional regularization term and what should be the
% value of $p$.

\input{theory}


