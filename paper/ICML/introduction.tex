%!TEX root=paper.tex

\section{Introduction}
{\bf Alternative intro sketch:}
As neural networks become commodity in many applications from improving camera quality on mobile phones \cite{googleapple} over language translation \cite{languagetranslation} to text auto-complete \cite{autocomplete}, inference performance and model size become equally important to the quality of the prediction.  
Surprisingly, today those three concerns, quality, performance and model size, are largely handled separately with suboptimal results. 
That is, models are trained with a fixed architecture (e.g., considering a certain size and performance budget), but then during the training the network architecture is considered fixed and does not further improve.
This is especially problematic for complex networks architectures such as CNNs where the search space of potentially network architectures is so big, that it is unfeasible to try all possible combinations. 
 
In contrast, recently proposed techniques to make neural nets smaller \cite{something1,something2} allow to start from a network architecture which is known to be perform well, but do not focus on the inference performance and often have a negative impact on the model quality.
In fact, besides universal applicable techniques such as quantitazation \cite{quant} and compiling neural networks to binary code to avoid unneccassary overhead, most existing techniques to make neural nets smaller actually harm performance as they often transform dense into sparse matrix multiplications, which is inherently slower to execute on modern hardware \cite{something}. 
%I am still not super happy with the flow. Will work on it more. 

In this paper we present a new method to automatically tune the network size while training the model to drastically improve the inference performance with comparable quality to much larger networks  
The key idea is to \emph{learn} the right network size at the same time that the network is learning the main task. 
For example, for an image classification task we can start with a well-known model architecture and shrink it while training the model  to end up with a much smaller network that has learned to classify images with an accuracy similar to larger network architectures.
Our approach has two main benefits. 
First, in contrast to existing neural network compression techniques, such as brain damage, it results in small dense models, which helps to improve inference performances
Second, we avoid the exhaustive and expensive model search to find a smaller network architecture with good accuracy. 

{\bf Old introduction:}
One of the key defactors that affects neural net perforamnce is  is the {\it shape} of the network, 
i.e., the number of layers, the number of neurons per layer, and the connections between
layers.
An {\it under-sized} network, with too few neurons or layers,
 is likely to have low accuracy because of insufficient 
capacity while an {\it over-sized} network is slow to train due to additional 
parameters and is computationally inefficient at both training and inference time.
% A sub-optimal network shape can lead to low accuracy .
%Although the hyperparameters related wo network size can dramatically affect
% performance, there is no reliable way to efficiently set them.
Consequently, many {\it hyperparameter optimization} techniques have been proposed to determine the optimal size of
a neural network; these include random search~\cite{paper-on-random-is-good-enough}, 
what-is-this-paper~\cite{Bengio2012a}, 
meta-gradient descent~\cite{Pedregosa2016},
Gaussian processes~\cite{Bergstra2011a}, and many others.
\srm{Are we really sure that no other techniques optimize the network size during training like we do?  Don't we also require 
iterating over lambda?  Is this really the key contribution of our work, that it reduces model search time?  Isn't the key point that
we find smaller, better models?}
These existing techniques all require a compute-intensive search of 
model space,  often training of tens or hundreds of models.
As a result, tuning these techniques require times 
longer (or many times more computational power) 
than the time take for a single training run.

In this paper we present a novel method to automatically find an appropriate
network size, which drastically reduces optimization time. \srm{In comparison to previous search models?}
The key idea is to learn the network size while optimizing the primary objective function.
Our strategy, called \textbf{ShrinkNets}, is blah blah \mpv{Short blurb about the technique}
% For example, for an image classification task, with our approach we can 
% provide the training data to a network—without sizing it a priori—and expect 
% to end up with a network that has learned to classify images with an accuracy
% similar to a the best manually engineered network. 
Our approach has two main benefits. 
First, we no longer need to choose a network size before training.
We simply set an initial size for the network and then the algorithm will
determine the best network that is smaller or equal in size to the inital size.
Second, this optimization is done during a {\it single} training run, as opposed to the large number of training runs
required by existing hyperparameter optimization 
techniques.  As a result, we can find the best model 
faster as well as with less computational overhead.

In summary, our contributions are as follows: 
\begin{compactenum}
\item We propose a novel technique based on dynamically swiching on and off neurons, 
which allows us to optimize the network size as the network is trained. 
\item We show that our technique is a relaxation\mpv{?} of group sparsity and 
prove \mpv{fill in}. 
\item \srm{Some claim about model size vs performance}
\item We demonstrate the efficacy of our technique on both convolutional and fully-connected neural nets,
showing that 
ShrinkNets finds networks within +/-X\% of best hand-crafted accuracy in XX\% of
training time compared to existing hyperparameter optimization methods.
\item We also demonstrate that ShrinkNets can achieve this accuracy with only YY\% 
of neurons.
\item \srm{Some claim about inference time?}
\item \srm{Some claim about compatibility with existing compression techniques?}
\end{compactenum}
