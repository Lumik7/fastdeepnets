%!TEX root=paper.tex

\section{Introduction}

Neural networks are increasingly being deployed in a wide variety  of
applications  and on diverse hardware architectures ranging from laptops to
phones to embedded sensors. This wide variety of deployment settings means
that inference time and model size are becoming as important as  prediction
quality when assessing model performance.  However, these three dimensions of
performance (model quality, inference time and model size)  are largely
optimized separately today, often with suboptimal results.

Of course, the problem of finding compressed or small networks is not new.
Existing techniques to make a pre-trained neural network smaller can be categorized in two approaches: (1) quantization \cite{quant} and code compilation~\cite{ma2016compilation}, 
techniques that can be applied blindly to any network, and (2) techniches which analyze the
structure of the network and systematically prune
connections or neurons~\cite{han2015deepcompression,Cun}
based on some loss function. While the first category is always useful, but only has limited 
impact on the network size, the second category can reduce the model size much more but has several 
drawbacks:   First, those techniques often negatively
impact model quality.  Second, they can (surprisingly) negatively impact
inference time as they transform dense matrix operations into sparse ones,
which can be substantially slower to execute on modern hardware
\cite{han2015deepcompression}, especially GPUs which do not efficiently support sparse
linear algebra.  Third, these techniques generally start by optimizing a
particular architecture for prediction performance, and then, as a post-
processing step, applying compression  to generate a smaller model that meets the
resource constraints of the deployment setting.  Because the network
architecture is essentially fixed during this post-processing,   model architectures
that work better in small settings may be missed -- this is especially true in
large networks like many-layered CNNs,  where it is infeasible to try explore
even a small fraction of possible network configurations.

%I am still not super happy with the flow. Will work on it more. 

In contrast, in this paper we present a new method to simultaneously optimize
 network size and model performance. The key idea is to learn the right
network size at the same time that we optimize for prediction performance for a
specific task. Our approach, called {\it ShrinkNets}, starts with an {\it
over-sized} network, and dynamically shrinks it by eliminating unimportant
neurons---those that do not contribute to prediction performance---during
training time. To do this, ShrinkNets must detect the unimportant neurons, and
remove them from the network. The
removal of entire neurons and not connections is crucial, because it leads to
dense networks, which in turn leads to better inference performance. To support the
detection and removal of unimportant neurons we introduce a new layer, called
\textbf{Switch layer}. Introducing this new layer simply requires us to
add a term to the
loss function to optimize the weights of this layer during training. ShrinkNets has
two main benefits. First, it explores the architecture of models that are both
small and perform well, rather than starting with a high-performance model and
making it small.  It does this using a single new hyperparameter that effectively 
models the target network size.  This allows us to efficiently generate a family of smaller and
accurate models without an exhaustive and expensive hyperparameter search over
the number of neurons in each layer.  Second, in contrast to existing neural
network compression techniques~\cite{Aghasi2016,han2015deepcompression}, our
approach results in models that are not only small, but where the layers are
dense, which means that inference time is also improved, e.g., on GPUs.





% {\bf Old introduction:}
% One of the key defactors that affects neural net perforamnce is  is the {\it shape} of the network, 
% i.e., the number of layers, the number of neurons per layer, and the connections between
% layers.
% An {\it under-sized} network, with too few neurons or layers,
%  is likely to have low accuracy because of insufficient 
% capacity while an {\it over-sized} network is slow to train due to additional 
% parameters and is computationally inefficient at both training and inference time.
% % A sub-optimal network shape can lead to low accuracy .
% %Although the hyperparameters related wo network size can dramatically affect
% % performance, there is no reliable way to efficiently set them.
% Consequently, many {\it hyperparameter optimization} techniques have been proposed to determine the optimal size of
% a neural network; these include random search~\cite{paper-on-random-is-good-enough}, 
% what-is-this-paper~\cite{Bengio2012a}, 
% meta-gradient descent~\cite{Pedregosa2016},
% Gaussian processes~\cite{Bergstra2011a}, and many others.
% \srm{Are we really sure that no other techniques optimize the network size during training like we do?  Don't we also require 
% iterating over lambda?  Is this really the key contribution of our work, that it reduces model search time?  Isn't the key point that
% we find smaller, better models?}
% These existing techniques all require a compute-intensive search of 
% model space,  often training of tens or hundreds of models.
% As a result, tuning these techniques require times 
% longer (or many times more computational power) 
% than the time take for a single training run.

% In this paper we present a novel method to automatically find an appropriate
% network size, which drastically reduces optimization time. \srm{In comparison to previous search models?}
% The key idea is to learn the network size while optimizing the primary objective function.
% Our strategy, called \textbf{ShrinkNets}, is blah blah \mpv{Short blurb about the technique}
% % For example, for an image classification task, with our approach we can 
% % provide the training data to a network—without sizing it a priori—and expect 
% % to end up with a network that has learned to classify images with an accuracy
% % similar to a the best manually engineered network. 
% Our approach has two main benefits. 
% First, we no longer need to choose a network size before training.
% We simply set an initial size for the network and then the algorithm will
% determine the best network that is smaller or equal in size to the inital size.
% Second, this optimization is done during a {\it single} training run, as opposed to the large number of training runs
% required by existing hyperparameter optimization 
% techniques.  As a result, we can find the best model 
% faster as well as with less computational overhead.

In summary, our contributions are as follows: 

\begin{compactenum}

\item We propose a novel technique based on dynamically switching on and off neurons, 
which allows us to optimize the network size while the network is trained. 

\item We implement our technique so as to remove entire neurons, therefore
leading not only to small and well-performing networks, but also dense, which
means higher inference performance. Furthermore, our switching layers used during training can be 
savely removed before the model is used for inference, leading to zero additional overhead. 

%\item \srm{xxx postprocessing techniques to strip out switch layers}

\item We show that our technique is a relaxation of group LASSO~\cite{Yuan2006}
and prove that our problem admits many global minima.

\item We demonstrate our claims with both fully-connected and convolutional well
known neural network architectures. We show that with ShrinkNets for
\texttt{CIFAR10} we achieve the same accuracy than a traditionally trained
network while reducing the network size by a factor of $2.2x$. Further, when
willing to sacrifice only $1$\% of performance, ShrinkNets finds networks which
are 35\% smaller. All in all, this leads to speedups in inference time of up to
40X. 

%\item We demonstrate the efficacy of our technique on both convolutional and
%fully-connected neural nets, showing that ShrinkNets finds networks within
%+/-X\% of best hand-crafted accuracy in XX\% of training time compared to
%existing hyperparameter optimization methods. \srm{Mention the specific
%model/dataset where this is true.}

%\item We demonstrate that ShrinkNets can achieve this accuracy with only YY\% of
%neurons. \srm{This claim should be something about how much smaller a network
%with say 99\% accuracy is one some real dataset}
%
%\item \srm{Some claim about inference time}
%
%\item \srm{Some claim about model size vs performance}

\end{compactenum}

% The remaining of this paper is organized as follows. In section
% \ref{sec:relatedwork} we discuss the related work. In section
% \ref{sec:approach}, we introduce our approach ShrinkNets, followed by the
% practical considerations we implemented to make it work in section
% \ref{sec:impl}. Then, we evaluate the approach in section \ref{sec:evaluation}
% and conclude the paper in section \ref{sec:conclusions}.


