%!TEX root=paper.tex

\section{Introduction}

One of the key defactors that affects neural net perforamnce is  is the {\it shape} of the network, 
i.e., the number of layers, the number of neurons per layer, and the connections between
layers.
An {\it under-sized} network, with too few neurons or layers,
 is likely to have low accuracy because of insufficient 
capacity while an {\it over-sized} network is slow to train due to additional 
parameters and is computationally inefficient at both training and inference time.
% A sub-optimal network shape can lead to low accuracy .
%Although the hyperparameters related wo network size can dramatically affect
% performance, there is no reliable way to efficiently set them.
Consequently, many {\it hyperparameter optimization} techniques have been proposed to determine the optimal size of
a neural network; these include random search~\cite{paper-on-random-is-good-enough}, 
what-is-this-paper~\cite{Bengio2012a}, 
meta-gradient descent~\cite{Pedregosa2016},
Gaussian processes~\cite{Bergstra2011a}, and many others.
\srm{Are we really sure that no other techniques optimize the network size during training like we do?  Don't we also require 
iterating over lambda?  Is this really the key contribution of our work, that it reduces model search time?  Isn't the key point that
we find smaller, better models?}
These existing techniques all require a compute-intensive search of 
model space,  often training of tens or hundreds of models.
As a result, tuning these techniques require times 
longer (or many times more computational power) 
than the time take for a single training run.

In this paper we present a novel method to automatically find an appropriate
network size, which drastically reduces optimization time. \srm{In comparison to previous search models?}
The key idea is to learn the network size while optimizing the primary objective function.
Our strategy, called \textbf{ShrinkNets}, is blah blah \mpv{Short blurb about the technique}
% For example, for an image classification task, with our approach we can 
% provide the training data to a network—without sizing it a priori—and expect 
% to end up with a network that has learned to classify images with an accuracy
% similar to a the best manually engineered network. 
Our approach has two main benefits. 
First, we no longer need to choose a network size before training.
We simply set an initial size for the network and then the algorithm will
determine the best network that is smaller or equal in size to the inital size.
Second, this optimization is done during a {\it single} training run, as opposed to the large number of training runs
required by existing hyperparameter optimization 
techniques.  As a result, we can find the best model 
faster as well as with less computational overhead.

In summary, our contributions are as follows: 
\begin{compactenum}
\item We propose a novel technique based on dynamically swiching on and off neurons, 
which allows us to optimize the network size as the network is trained. 
\item We show that our technique is a relaxation\mpv{?} of group sparsity and 
prove \mpv{fill in}. 
\item \srm{Some claim about model size vs performance}
\item We demonstrate the efficacy of our technique on both convolutional and fully-connected neural nets,
showing that 
ShrinkNets finds networks within +/-X\% of best hand-crafted accuracy in XX\% of
training time compared to existing hyperparameter optimization methods.
\item We also demonstrate that ShrinkNets can achieve this accuracy with only YY\% 
of neurons.
\item \srm{Some claim about inference time?}
\item \srm{Some claim about compatibility with existing compression techniques?}
\end{compactenum}