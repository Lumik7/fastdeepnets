%!TEX root=paper.tex

\section{Introduction}

As neural networks become increasingly widely deployed in a variety of
applications  -- ranging from improving camera quality on mobile phones
\cite{googleapple} to language translation \cite{languagetranslation} to text
auto-completion \cite{autocomplete} -- and on diverse hardware architectures
-- from laptops to phones to embedded sensors --  inference performance and
model size are becoming as important in learning as metrics measures of
prediction quality. However, these three aspect of model performance --
quality, performance and size -- are largely optimized separately today, often
with suboptimal results.

Of course, the problem of finding compressed or small networks is not new.
Existing techniques typically aim make a pre-trained neural network smaller
\cite{something1,something2} by taking one of two approaches:  either by
applying quantization \cite{quant} or code compilation~\cite{something}
techniques that can be applied blindly to any network, or by analyzing the
structure of the network and systematically pruning
connections~\cite{han2015deepcompression,Cun} or neurons~\cite{XXX}
based on some loss function.  Although these techniques can substantially
reduce model size, they have several drawbacks.  First, they often negatively
impact model quality.  Second, they can (surprisingly) negatively impact
inference time as they transform dense matrix operations into sparse ones,
which can be substantially slower to execute on modern hardware
\cite{something}, especially GPUs which do not efficiently support sparse
linear algebra.  Third, these techniques generally start by optimizing a
particular architecture for prediction performance, and then, as a post-
processing step, applying compression  to generate a smaller model that meets the
resource constraints of the deployment setting.  Because the network
architecture is essentially fixed during this post-processing,   model architectures
that work better in small settings may be missed -- this is especially true in
large networks like many-layered CNNs,  where it is infeasible to try explore
even a small fraction of possible network configurations.

%I am still not super happy with the flow. Will work on it more. 

In contrast, in this paper we present a new method to simultaneously optimize
both network size and model performance. The key idea is to learn the right
network size at the same time that we optimize for prediction performance. Our
approach, called {\it ShrinkNets}, starts with an {\it oversized} network, and
dynamically shrinks it by eliminating neurons during training time.  Our
approach has two main benefits.  First, we explore the architecture of models
that are both small and perform well, rather than starting with a high-
performance model and making it small.  This allows us to efficiently generate
a family of smaller and accurate models without an exhaustive and expensive
hyperparameter search over the number of neurons in each layer. Second, in
contrast to existing neural network compression techniques~\cite{Aghasi2016,han2015deepcompression}, our approach results in models that are
not only small, but where the  layers are dense, which means that inference
time is also improved, e.g., on GPUs.


% {\bf Old introduction:}
% One of the key defactors that affects neural net perforamnce is  is the {\it shape} of the network, 
% i.e., the number of layers, the number of neurons per layer, and the connections between
% layers.
% An {\it under-sized} network, with too few neurons or layers,
%  is likely to have low accuracy because of insufficient 
% capacity while an {\it over-sized} network is slow to train due to additional 
% parameters and is computationally inefficient at both training and inference time.
% % A sub-optimal network shape can lead to low accuracy .
% %Although the hyperparameters related wo network size can dramatically affect
% % performance, there is no reliable way to efficiently set them.
% Consequently, many {\it hyperparameter optimization} techniques have been proposed to determine the optimal size of
% a neural network; these include random search~\cite{paper-on-random-is-good-enough}, 
% what-is-this-paper~\cite{Bengio2012a}, 
% meta-gradient descent~\cite{Pedregosa2016},
% Gaussian processes~\cite{Bergstra2011a}, and many others.
% \srm{Are we really sure that no other techniques optimize the network size during training like we do?  Don't we also require 
% iterating over lambda?  Is this really the key contribution of our work, that it reduces model search time?  Isn't the key point that
% we find smaller, better models?}
% These existing techniques all require a compute-intensive search of 
% model space,  often training of tens or hundreds of models.
% As a result, tuning these techniques require times 
% longer (or many times more computational power) 
% than the time take for a single training run.

% In this paper we present a novel method to automatically find an appropriate
% network size, which drastically reduces optimization time. \srm{In comparison to previous search models?}
% The key idea is to learn the network size while optimizing the primary objective function.
% Our strategy, called \textbf{ShrinkNets}, is blah blah \mpv{Short blurb about the technique}
% % For example, for an image classification task, with our approach we can 
% % provide the training data to a network—without sizing it a priori—and expect 
% % to end up with a network that has learned to classify images with an accuracy
% % similar to a the best manually engineered network. 
% Our approach has two main benefits. 
% First, we no longer need to choose a network size before training.
% We simply set an initial size for the network and then the algorithm will
% determine the best network that is smaller or equal in size to the inital size.
% Second, this optimization is done during a {\it single} training run, as opposed to the large number of training runs
% required by existing hyperparameter optimization 
% techniques.  As a result, we can find the best model 
% faster as well as with less computational overhead.

In summary, our contributions are as follows: 
\begin{compactenum}
\item We propose a novel technique based on dynamically switching on and off neurons, 
which allows us to optimize the network size as the network is trained. 
\item We show that our technique is a relaxation of group LASSO~\cite{Yuan2006} and prove that our problem admits many global minima.
\item \srm{Some claim about model size vs performance}
\item We demonstrate the efficacy of our technique on both convolutional and fully-connected neural nets,
showing that 
ShrinkNets finds networks within +/-X\% of best hand-crafted accuracy in XX\% of
training time compared to existing hyperparameter optimization methods. \srm{Mention the specific 
model/dataset where this is true.}
\item We demonstrate that ShrinkNets can achieve this accuracy with only YY\% 
of neurons. \srm{This claim should be something about how much smaller a network with say 99\%
accuracy is one some real dataset}
\item \srm{Something about dense networks and then benefit over optimal brain damage}
\item \srm{Some claim about inference time}
\item \srm{Some claim about compatibility with existing compression techniques?}
\end{compactenum}
