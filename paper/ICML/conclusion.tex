\section{Conclusion}

We presented ShrinkNets, an approach to learn deep network sizes while training.
ShrinkNets consists of the Switch Layer, which deactivates neurons, as well as
of a method to remove them, which shrinks network sizes leading to faster
inference times. We demonstrated these claims on on two well-known datasets, for
which we achieved networks of the same accuracy than traditional neural
networks, but up to 35X smaller; all in all leading to inference speedups of up
to 40X.

