\section{Related Work}

\begin{itemize}
  \item Hyperparameter optimization: random, bayesian opt, bandit methods
  \item distillation techniques
  \item post-training compression techniques
  \item group sparsity, non-parametric neural networks
  \item training dynamics paper: first overfitting and then randomization?
\end{itemize}

Given the importance of network structure, many techniques have been proposed to
find the best network structure for a given learning task.
These techniques broadly fall into four categories: hyperparameter optimization
strategies, post-training model compression for inference as well as model 
simplification, techniques to resize models during training, and 
automated architecture search methods.

The most popular techniques for hyperparameter optimization include simple 
methods such as random search~\cite{BergstraJAMESBERGSTRA2012} which have been shown to work
suprisingly well compared to more complex methods such as those based on
Bayesian optimization~\cite{Snoek12}.
Techniques such as~\cite{Snoek12} model generalization performance via 
Gaussian Processes~\cite{GaussianProcesses} and select hyperparameter 
combinations that come from uncertain areas of the hyperparameter space.
Recently, methods based on bandit algorithms (e.g. ~\cite{li2016hyperband, 
jamieson2016}) have also become a popular way to tune hyperparameters.
As noted before, all of the above techniques require many tens to hundreds of
models to be trained, making this process computationally inefficient and slow.

In contrast with hyperparameter tuning methods, some methods such as 
DeepCompression~\cite{han2015deepcompression} seek to compress the network to 
make inference more efficient.
This is accomplished by pruning connections and quantizing weights.
On similar lines, multiple techniques such as~\cite{romero2014fitnets, 
hinton2015distilling} have been proposed for distilling a network into a 
simpler network or a different model.
Unlike our technique which works during training, these techiques are used after
training and it would be interesting to apply them to ShrinkNets as well.

The techniques closed to our work are those based on group sparsity such as 
\mpv{Guillaume: fill in}.

Finally, there has also been recent work in automatically learning model
architecture through the use of genetic algorithms and reinforcement learning 
techniques~\cite{DBLP:journals/corr/ZophL16, zoph2017learning}.
These techniques are focused more on learning higher-level architectures (e.g.
building blocks for neural network architectures) as opposed to learning 
network size.
