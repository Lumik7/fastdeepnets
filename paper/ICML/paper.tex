%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{stmaryrd}
\usepackage{booktabs} % for professional tables
\usepackage{paralist}
\usepackage{xspace}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{amsmath}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\intint}[1]{\left\llbracket#1\right\rrbracket}
\newtheorem{proposition}{Proposition}[section]


% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}
\newcommand{\srm}[1]{\textcolor{red}{{\bf Sam:} #1}}
\newcommand{\tim}[1]{\textcolor{red}{{\bf Tim:} #1}}
\newcommand{\ra}[1]{\textcolor{blue}{{\bf ra:} #1}}
\newcommand{\gl}[1]{\textcolor{violet}{{\bf Gl:} #1}}
\newcommand{\mpv}[1]{\textcolor{green}{{\bf MV:} #1}}
\newcommand{\shrink}{{\bf ShrinkNets}\xspace}
\newcommand{\swl}{{\bf SwitchLayer}\xspace}
\newcommand{\swls}{{\bf SwitchLayers}\xspace}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{ShrinkNets: Learning Network Size while Training}

\begin{document}

\twocolumn[
\icmltitle{ShrinkNets: Learning Network Size while Training}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Guillaume Leclerc}{mit,epfl}
\icmlauthor{Raul Castro Fernandez}{mit}
\icmlauthor{Manasi Vartak}{mit}
\icmlauthor{Martin Jaggi}{epfl}
\icmlauthor{Samuel Madden}{mit}
\end{icmlauthorlist}

\icmlaffiliation{mit}{Computer Science and Artificial Intelligence Laboratories, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA}

\icmlaffiliation{epfl}{Swiss Federal Institute of Technology, Lausanne, Switzerland}

\icmlcorrespondingauthor{Guillaume Leclerc}{leclerc@mit.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
As neural networks become widely deployed in different applications and 
on different
hardware, it has become increasingly important to optimize inference time and
model size along with model accuracy.  
Most current techniques optimize model size,
model accuracy and inference time in different stages, resulting in suboptimal
results and computational inefficiency.  
In this work, we propose a new
technique called \shrink that optimizes all three of these metrics
simultaneously.  
Specifically we present a new method to simultaneously optimize network size and
model performance by neuron-level pruning during training.  
Neuron-level pruning not only
produces much smaller networks for a given accuracy threshold but also produces
dense weight matrices that are amenable to efficient inference.  
By applying our
technique to convolutional as well as fully connected models, we show that 
\shrink can reduce network size by 35X with a 6X improvement in inference time.
In addition, the most accurate networks found by \shrink have accuracy that is
similar to or better than those found by statically-sized networks.  
\end{abstract}

%\section{Tentative outline}
%
% \begin{itemize}
  % \item We want to figure out what is the proper network size
  % \item If we had a oversized network and for each neuron we would have an on/off switch, we could optimize the state of each switch to achieve any size/accuracy tradeof (we can prove that, do we care ?)
  % \item Solving such problem is NP-Hard
  % \item We could approximate the binary switch by relaxing them and doing L1 loss instead of L0 loss
  % \item This is the definition of Shrinknets
  % \item group sparsity tries to achieve a similar goal but with a different formulation
  % \item How are they related ?
  % \item If we add a specific constraint on our formulation we obtain the group sparsity one (proven)
  % \item We conclude that without this constraint our formulation has more degree of freedom
  % \item What happen when we drop this constraint
  % \item The problem becomes non-convex and without a global minimum
  % \item Not having a global minimum is bad, how can we solve that ?
  % \item Adding an extra regularization parameter allow us to have a global minimum (a lot of them actually)
  % \item Now we have a framework to switch on and of neurons
  % \item We are still doing useless computation for neurons that will stay off forever, if we pruned them then we would speed up training
  % \item Define the strategy to detect "forever dead" neurons
  % \item (should we talk about the software architecture ?)
  % \item now we evaluate the system
  % \item First we show that our system has indeed more freedom than Group sparsity by showing that for any given amount of sparsity it can fit closer than the previous method
  % \item On the same problem we show that removing neurons on the fly does not dramatically reduce accuracy (we might not be able to see any difference in performance for a linear/logistic regression though)
  % \item We show that works with Bigger networks (multi layers perceptrons and CNNs), describe the training dynamics, discuss the shape we obtain
  % \item Show that it is easier to do hyper-parameter optimization on the regularization parameter that the size of each layer. I think a nice experiment would be to plot the evolution of the final size of a two layer network (so going from a lambda to two sizes). And plot the trajectory in the "size space" as we decrease lambda. It would show how the system trades off the budget between the two layer in a nice and visual way
  % \item Then we evaluate performance
% \end{itemize}

% \section{Outline2}
%\begin{itemize}
%  \item Introduction
%  \begin{itemize}
%    \item Finding an appropriately sized network is challenging
%    \item ML practitioners spend a large amount of time tuning the size of layers
%    in a network in order to get the best possible accuracy.
%    \item Many techniques have been proposed for hyperparam opt but these
%    are computationally expensive and take long to train
%    \item We propose ShrinkNets, an approach to tune the size of the network 
%    as it is being trained without incurring the significant costs of 
%    hyperparameter optimization. 
%    \item Thus, our contribution are: ...
%  \end{itemize}
%  \item Our Approach
%  \begin{itemize}
%    \item Assign an on/off switch to each neuron. But this is np-hard, so we
%    consider a relaxation
%    \item Formulation
%    \item Relationship to sparsity
%    \item Strategies to kill neurons (relation to theory above?)
%    \item Implementation details
%  \end{itemize}
%  \item Related Work
%  \begin{itemize}
%    \item Hyperparameter optimization: random, bayesian opt, bandit methods
%    \item distillation techniques
%    \item post-training compression techniques
%    \item group sparsity, non-parametric neural networks
%    \item training dynamics paper: first overfitting and then randomization?
%  \end{itemize}
%  \item Experiments
%  \begin{itemize}
%    \item Accuracy obtained by Shrinknets
%    \item Time taken to reach that accuracy compared with other hyperopt methods
%    \item Characterize method wrt params
%    \item Other experiments
%  \end{itemize}
%  \item Discussion
%  \begin{itemize}
%    \item Where does the method shine?
%    \item Side-effects of method: smaller networks, time to train
%    \item Potential extensions/limitations
%  \end{itemize}
%  \item Conclusion
%\end{itemize}

\input{introduction}
\input{related_work}
\input{approach}
\input{impl}
\input{evaluation}
%\input{discussion}
\input{conclusion}

\appendix
\input{appendix}
\FloatBarrier

\bibliographystyle{icml2018}
\bibliography{custom,general}



\end{document}
