\section{ShrinkNets in Practice}

\subsubsection{Neuron killing}
Once Switch layers are placed in a network and
initialized (sampled from the $\mathcal{N}(0, 1)$ distribution),

\textbf{Threshold strategy}:  We kill neurons based on a threshold (in absolute
value), this is the method used by deep compression. It is bad because it is
scale dependant and is not robust to noise in the gradients (explain that)
\\ \textbf{Sign change strategy}:  We look at when the sign change. This is also
sensitive to noise but has the advantage
\textbf{Sign variance strategy}: We measure the variance of the sign ($-1, 1$) of each component in the \textit{Switch Layer}, and consider it dead when it goes over a predefined threshold. \gl{Maybe we should add some intuition on why it makes sense}. \gl{We should probably explain the impact of $\gamma$ and report numbers from the expertiment that show that it does not hurt too much to kill neurons, we have the data in this experiment in the evaluation that we probably want to remove later.}

\subsection{Implementation}

We have implemented \textbf{Switch Layers} and the associated training procedure
as a library in pytorch~\cite{github}.
The layer can be freely mixed with other popular layers such as convolutional
layers, batchnorm layers, fully connected layers etc.
There are two key challenges that must be overcome while implementing 
\textbf{Switch Layers}.
First, since ShrinkNets kill off a large fraction of neurons, we must remove the
overhead of carrying around dead neurons.
And second, we must update the various network layers and training machinery (e.g.
optimizer state) to reflect the deactivation of neurons.
We address these challenges by augmenting every layer with additional information
about neuron status and implementing a \emph{neural garbage collection} 
mechanism which prunes deactivated neurons on-the-fly and updates the state of 
the training machinery.
% We do so by augmenting every layer with additional information such as what
% neurons have been deactivated.
% The layer then updates its size accordingly and this change is then propagated
% to other parts of the training machinery including the optimizer which must
% know the sizes of parameters.

A second implementation challenge we address with ShrinkNets is the potentially
higher inference time.
This increase in inference time is both due to the extra computation introduced 
by switch layers and because our switch layers are not currently optimized for 
CUDA.
Therefore, for now, our library provides a post-training routine to ``fold''
switch layers into existing, optimized layers.
Specifically, for every switch layer, we check if the parent or child of this 
layer is a linearly scalable layer (e.g. convolutional layer, fully connected layer,
batchnormalization layer) and if so, we fold the switch layer into this 
neighboring layer by multiplying through by the switch weights.
In the unlikely case that a switch layer is sandwiched between two non-linearly
scalable layers, we leave the switch layer as is.


% \subsection{Speeding up training}

% In order to have a practical implementation of ShrinkNets it is absolutely
% necessary to prune useless neurons as quick as possible. In order to try more
% different strategies we split this task in two subproblems: detecting
% potentially non-crucial neurons and setting their corresponding entry in the
% switch layer to exactly 0, and neural garbage collection that 



% In our library we implemented a variant of the second strategy and the last one.

% \subsubsection{Neural garbage collection}




% It is possible to reduce the overhead of the training process by removing
% neurons as soon as they become deactivated by $\theta$ going to 0.
%If
%disabled neurons are not quickly removed, the overhead might cause the training
%process to be significantly slower than classic neural networks. 
% To do this, we implemented a \emph{neural garbage collection} mechanism which
% prunes deactivated neurons on-the-fly,  reducing the processing time
% and memory overhead. 
% To support this feature, it is crucial to understand the
% information flow between neurons and layers in the neural network. 
% We achieve
% this by representing such information flow as a graph. 
% Vertices represent layers,
% and edges are event-hubs responsible for propagating information about disabled
% neurons to the relevant layers. 
% As soon as a layer receive an event, it updates
% his parameters to reflect the new size. 
% Each operation is stored in a log that
% is used later to update other component of the system. 
% For example some
% optimizers like Adam [Ref] store state about each parameter, their state needs
% to be updated the exact same way and at the exact same time as layers in order
% to obtain apropriate parameter updates.

% \subsubsection{Speeding up Inference}
% \mpv{This may be moved to discussion or ``other considerations'' or impl details.}



% our post-processing code searches the 
% network graph to identify a parent or child layer the linear layer closest (child or parent) to every 
% switch layer and multiplies the weights of that layer by the corresponding
% switch layer weights.


% Our proposed \textit{Switch Layer} are relevant only during training, and can be
% removed from the network architecture during inference if they can be ``folded''
% into the nearest linear layer.
% Specifically, we provide a post-processing routine that will, for each switch 
% layer, search the network graph 

% The sole puropose of \textit{Switch Layer} are to detect which neurons should be
% killed and which should be left. When the training is over we will never remove
% more neurons. Therefore, they loose all their interest at inference time. 
% To
% improve size and inference time we propose the following method to get rid of
% them without changing the output of the model (modulo floating point errors).
% The basic idea is to start from every \textit{Switch Layer} and try to find the
% nearset linear operator (child or parent) and multiply its weights by the values
% in the switch layer. For example we can merge Filter layers with surrounding
% Convolutional layer, Linear layer or even BatchNormalization [ref] if they are
% before the \textit{Switch Layer}. However we need to be extremely careful not to
% cross a non-linearity otherwise it would change the output of network.
